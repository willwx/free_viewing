{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f063dc-f125-4ee5-8a9f-dabf4ad6ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "from warnings import warn\n",
    "\n",
    "import cv2\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../lib')\n",
    "from modeling import models, registry\n",
    "from modeling.utils import make_layer_hook, recur_collapse_feats\n",
    "from storage import get_storage_functions\n",
    "from local_paths import stim_dir, cache_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d548d2ed-0e57-4175-bb1c-0883a0303266",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf321ad-26b1-4d81-8696-8ef82d773ac7",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# image to process\n",
    "#============================================================================\n",
    "im_md5s    = 'md5_im1,md5_im2'\n",
    "sep        = ','\n",
    "im_w       = 16    # size of full image; ...\n",
    "im_h       = 16    # unit: dva, but only ratio (im_size/patch_size) really matters\n",
    "ar_tol     = 3/4   # aspect ratio tolerance (between image file and provided size)\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# patch size and resolution\n",
    "#============================================================================\n",
    "patch_size =  2    # size of each crop patch\n",
    "patch_step =  0.5  # step size of patch location\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# model params\n",
    "#============================================================================\n",
    "model_name    = 'vit_large_patch16_384'\n",
    "layer_name    = 'blocks.13.attn.qkv'\n",
    "spatial_averaging = True  # over W, H for conv; over S for attention\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# paths\n",
    "#============================================================================\n",
    "# unlike other scripts, this one is intentionally unaware of subfolders\n",
    "# (thereby requiring image IDs, e.g., MD5s, to truly be unique)\n",
    "# all images are in [stim_dir]/Stimuli), so specify it explicitly\n",
    "stim_dir = stim_dir + 'Stimuli/'\n",
    "\n",
    "output_root = cache_dir + 'feats/'\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# misc\n",
    "#============================================================================\n",
    "device = 'cuda:0'\n",
    "bgc = (128,128,128)   # background color; used to pad images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f250a1-7a71-4412-a41b-bed7c78ecf61",
   "metadata": {},
   "source": [
    "# Check prereqs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2651528e-f12c-4741-a403-ef0dd37e47b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Loading images from folder', stim_dir)\n",
    "stim_dir = Path(stim_dir).expanduser()\n",
    "assert stim_dir.is_dir()\n",
    "\n",
    "output_root = Path(output_root)\n",
    "output_path = output_root / model_name / layer_name / \\\n",
    "    f'{im_w:.1f}x{im_h:.1f}_as_{patch_size}x{patch_size}_in_{patch_step:.2f}_steps.h5'\n",
    "print('Saving results to', output_path)\n",
    "output_path = output_path.expanduser()\n",
    "output_path.parent.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a637be-a896-479d-94e6-e27e3cb4b28a",
   "metadata": {},
   "source": [
    "# Prepare parameters; save config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb4b699a-c203-4b6a-b3aa-80a1dbabef1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_md5s = im_md5s.split(sep)\n",
    "print('Processing', len(im_md5s), 'images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d92c28-cad7-45ad-8c39-2a4afad63f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_size = (im_w, im_h)\n",
    "ar_tol = min(ar_tol, 1/ar_tol)\n",
    "patch_step = float(patch_step)\n",
    "\n",
    "# interpret this as origin == lower left\n",
    "patches_ledge_x = np.arange(int(np.ceil(im_size[0]/patch_step))) * patch_step  # full and right partial patches\n",
    "patches_ledge_x = np.concatenate([\n",
    "    np.arange(-1, -int(np.ceil(patch_size/patch_step)), -1)[::-1] * patch_step,  # left partial patches\n",
    "    patches_ledge_x])\n",
    "patches_ledge_x -= im_size[0] / 2  # align to image center\n",
    "n_patches_x = len(patches_ledge_x)\n",
    "\n",
    "patches_ledge_y = np.arange(int(np.ceil(im_size[1]/patch_step))) * patch_step  # full and right partial patches\n",
    "patches_ledge_y = np.concatenate([\n",
    "    np.arange(-1, -int(np.ceil(patch_size/patch_step)), -1)[::-1] * patch_step,  # left partial patches\n",
    "    patches_ledge_y])\n",
    "patches_ledge_y -= im_size[1] / 2  # align to image center\n",
    "n_patches_y = len(patches_ledge_y)\n",
    "\n",
    "print('Patches step size:', patch_step)\n",
    "print(f'Number of patches: {n_patches_x} x {n_patches_y} (x-by-y)')\n",
    "print('Patches (bin lower edge):')\n",
    "print('(The coordinates in degrees are with origin at image center)')\n",
    "print('x:')\n",
    "print('\\t' + str(patches_ledge_x).replace('\\n', '\\n\\t'))\n",
    "print('y:')\n",
    "print('\\t' + str(patches_ledge_y).replace('\\n', '\\n\\t'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213ad700-641e-43c5-aec2-89b23956a6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_imsize = registry.get_default_preproc(model_name)['imsize']\n",
    "print('Model input image size:', model_imsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3420d1f-cda1-4493-9202-0b59d9f0f3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tqdm_ = lambda x: tqdm(x, mininterval=300, miniters=10)  # to avoid bloated log file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da33ca34-2bb7-4c3b-ae05-e47f6ddbaeea",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results, add_attr_to_dset, check_equals_saved, link_dsets, copy_group = \\\n",
    "    get_storage_functions(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aeccaa-b760-4690-baf0-f6d3bc24af4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results('config/stimuli/size_dva', im_size)\n",
    "\n",
    "group = 'config/patch_grid/'\n",
    "save_results(group+'size', patch_size)\n",
    "save_results(group+'step', patch_step)\n",
    "save_results(group+'eft_edges', patches_ledge_x)\n",
    "save_results(group+'right_edges', patches_ledge_x+patch_size)\n",
    "save_results(group+'lower_edges', patches_ledge_y)\n",
    "save_results(group+'upper_edges', patches_ledge_y+patch_size)\n",
    "save_results(group+'x_locs', patches_ledge_x+patch_size/2)\n",
    "save_results(group+'y_locs', patches_ledge_y+patch_size/2)\n",
    "\n",
    "group = 'config/modelling/'\n",
    "save_results(group+'model_name', model_name)\n",
    "save_results(group+'layer_name', layer_name)\n",
    "save_results(group+'input_image_size', model_imsize)\n",
    "save_results(group+'spatial_averaging', spatial_averaging)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3d16a1-fff2-4f9f-8569-06f1fe82d2d3",
   "metadata": {},
   "source": [
    "# Locate & load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c8893e1-0605-44f5-a160-e1b20593131e",
   "metadata": {},
   "outputs": [],
   "source": [
    "done_md5s = None\n",
    "offset = 0\n",
    "if output_path.is_file():\n",
    "    with h5.File(output_path, 'r') as f:\n",
    "        try:\n",
    "            done_md5s = f['md5'][()].astype(str)\n",
    "            offset = len(done_md5s)\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "if done_md5s is not None:\n",
    "    done_md5s = set(done_md5s)\n",
    "    print(len(done_md5s), 'images already done')\n",
    "    im_md5s = [v for v in im_md5s if v not in done_md5s]\n",
    "    print('Processing', len(im_md5s), 'remaining images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6466728-85a7-41c5-9c7f-294ce254083b",
   "metadata": {},
   "outputs": [],
   "source": [
    "im_paths = [next(stim_dir.glob(md5+'.*')) for md5 in im_md5s]\n",
    "assert all(p.is_file() for p in im_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3471d324-5fa4-464d-af17-4ad3bde5f89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = im_size[0] / im_size[1]\n",
    "print(f'Defined image aspect ratio: {ar:.2f}')\n",
    "\n",
    "n_ims = len(im_paths)\n",
    "images = np.empty(n_ims, dtype=object)\n",
    "\n",
    "for iim, fp in enumerate(im_paths):\n",
    "    image = Image.open(fp)\n",
    "\n",
    "    # check image aspect ratio\n",
    "    ar_ = image.size[0] / image.size[1]\n",
    "    if not (0.99 < ar_/ar < 1.01):\n",
    "        if not (ar_tol < ar_/ar < 1/ar_tol):\n",
    "            warn(\n",
    "                f'image {fp.name} (size: {image.size}; AR = {ar_:.2f}) '\n",
    "                f'is very far from expected aspect ratio (size: {im_size}; AR = {ar:.2f} '\n",
    "                '(resizing it regardless, beceause it would have been presented at the '\n",
    "                'specified size)')\n",
    "\n",
    "        i = np.argmin(image.size[:2] / np.array(im_size))\n",
    "        if i == 0:\n",
    "            w = image.size[0]\n",
    "            h = int(round(w / ar))\n",
    "        else:\n",
    "            h = image.size[1]\n",
    "            w = int(round(h * ar))\n",
    "        print(f'Resizing {fp.name} (size: {image.size}; AR = {ar_:.2f}) to size {(h,w)} (AR = {w/h:.2f})')\n",
    "        image = np.array(image.resize((w, h)))\n",
    "    else:\n",
    "        image = np.array(image)\n",
    "\n",
    "    # make ims 8-bit RGB\n",
    "    assert image.dtype == np.uint8\n",
    "    if image.ndim == 3:\n",
    "        assert image.shape[-1] in (3,4)\n",
    "        if image.shape[-1] == 4:\n",
    "            image = image[:,:,:3]\n",
    "    else:\n",
    "        assert image.ndim == 2\n",
    "        image = np.repeat(image[:,:,None], 3, axis=-1)\n",
    "\n",
    "    images[iim] = image\n",
    "\n",
    "print(len(images), 'images')\n",
    "images.shape, images.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e3f48-b7a4-4f5e-a786-a158af96a85b",
   "metadata": {},
   "source": [
    "# Prepare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab177d7-f3f9-44aa-b02c-3103367b1bcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when no images to process, save time by not loading model\n",
    "# (unfortunately, I do not know how to early-stop an ipynb from within itself)\n",
    "if len(im_md5s):\n",
    "\n",
    "    model = models.get_model_by_name(model_name, dev=device)\n",
    "    preprocessing_func = models.get_preprocessor_by_model_name(model_name, preproc_imsize=False, preproc_from='numpy')\n",
    "\n",
    "    class Embedder:\n",
    "        def __init__(\n",
    "                self, model=model, preproc_fun=preprocessing_func,\n",
    "                model_name=model_name, layer_names=layer_name,\n",
    "                spatial_averaging=spatial_averaging,\n",
    "                fwd_fun=None, device=device, pbar=tqdm_):\n",
    "\n",
    "            self.model = model\n",
    "            self.preproc_fun = preproc_fun\n",
    "            self.spatial_averaging = spatial_averaging\n",
    "            self.device = device\n",
    "            self.pbar = pbar\n",
    "\n",
    "            if isinstance(layer_names, str):\n",
    "                layer_names = (layer_names,)\n",
    "            else:\n",
    "                assert all(isinstance(n, str) for n in layer_names)\n",
    "            self.layer_names = layer_names\n",
    "\n",
    "            if fwd_fun is None:\n",
    "                if model_name is not None and 'CLIP' in model_name:\n",
    "                    fwd_fun = model.encode_image\n",
    "                else:\n",
    "                    fwd_fun = model.__call__\n",
    "            self.fwd_fun = fwd_fun\n",
    "\n",
    "            hooks = {}\n",
    "            hdls = {}\n",
    "            for n in layer_names:\n",
    "                hooks[n], hdls[n] = make_layer_hook(model, n, return_handle=True)\n",
    "            self.hooks = hooks\n",
    "            self.hdls = hdls\n",
    "\n",
    "        def extract_pooled_features(self, ims):\n",
    "            feats = {n: [] for n in self.layer_names}\n",
    "            with torch.no_grad():\n",
    "                for im in self.pbar(ims):\n",
    "                    tim = self.preproc_fun(im).unsqueeze(0).to(self.device)\n",
    "                    self.fwd_fun(tim)\n",
    "\n",
    "                    for n, hook in self.hooks.items():\n",
    "                        feats_ = recur_collapse_feats(hook.o, spatial_averaging=self.spatial_averaging)\n",
    "                        if not isinstance(feats_, torch.Tensor):\n",
    "                            raise ValueError(f'unexpected feature type {type(feats_)} at layer {n}: {feats_}')\n",
    "                        feats_ = feats_.cpu().numpy()\n",
    "                        feats[n].append(feats_)\n",
    "\n",
    "            return {n: np.array(v) for n, v in feats.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eecd0e-3a6a-46ef-92a6-11a233ab8d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(im_md5s):\n",
    "    embedder = Embedder()\n",
    "    test_im = np.full((model_imsize, model_imsize, 3), bgc, dtype=np.uint8)\n",
    "\n",
    "    feats = embedder.extract_pooled_features([test_im])\n",
    "    feats = feats[layer_name][0]\n",
    "    print('feats:', feats.shape, feats.dtype)\n",
    "    sample_feats = feats\n",
    "    feats_shape = sample_feats.shape\n",
    "    save_results('config/modelling/pooled_feat_shape', sample_feats.shape)\n",
    "\n",
    "    with h5.File(output_path, 'a') as f:\n",
    "        if 'feats/bg' not in f:\n",
    "            save_results('feats/bg', sample_feats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4521dd6-7ab0-4cdb-baea-5a8c31159c26",
   "metadata": {},
   "source": [
    "# Initialize result storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328aa2fb-3ab5-4598-a4a6-bda0a712de8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ignoring_existing(f, *args, attrs=None, **kwargs):\n",
    "    assert isinstance(args[0], str)\n",
    "    try:\n",
    "        dset = f.create_dataset(*args, **kwargs)\n",
    "        if attrs is not None:\n",
    "            assert isinstance(attrs, dict)\n",
    "            for k, v in attrs.items():\n",
    "                dset.attrs[k] = v\n",
    "    except ValueError as e:\n",
    "        if 'name already exists' not in str(e):\n",
    "            raise\n",
    "        dset = f[args[0]]\n",
    "        if attrs is not None:\n",
    "            assert isinstance(attrs, dict)\n",
    "            for k, v in attrs.items():\n",
    "                check_equals_saved(v, dset.attrs[k], k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7102240-51d1-4667-80a2-25666335b5bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_opts = dict(compression='gzip', compression_opts=9)\n",
    "if len(im_md5s):\n",
    "    with h5.File(output_path, 'a') as f:\n",
    "        create_ignoring_existing(\n",
    "            f, 'md5',\n",
    "            shape=(0,),\n",
    "            maxshape=(None,),\n",
    "            chunks=(1,),\n",
    "            dtype='S32',\n",
    "            **cache_opts)\n",
    "\n",
    "        dims = np.array(['image', 'feat_chan'], dtype=bytes)\n",
    "        coords = np.array(['md5', 'feat_chans'], dtype=bytes)\n",
    "        create_ignoring_existing(\n",
    "            f, 'feats/full_image',\n",
    "            shape=(0,)+feats_shape,\n",
    "            maxshape=(None,)+feats_shape,\n",
    "            attrs=dict(dims=dims, coords=coords),\n",
    "            chunks=(1,)+feats_shape,\n",
    "            dtype=sample_feats.dtype,\n",
    "            **cache_opts)\n",
    "\n",
    "        dims = np.array(['image', 'rf_x', 'rf_y', 'feat_chan'], dtype=bytes)\n",
    "        coords = np.array(['md5', 'patch_locs', 'patch_locs', 'feat_chans'], dtype=bytes)\n",
    "        shape_ = (n_patches_x, n_patches_y,) + feats_shape\n",
    "        create_ignoring_existing(\n",
    "            f, 'feats/patch_grid',\n",
    "            shape=(0,)+shape_,\n",
    "            maxshape=(None,)+shape_,\n",
    "            attrs=dict(dims=dims, coords=coords),\n",
    "            chunks=(1,)+shape_,\n",
    "            dtype=sample_feats.dtype,\n",
    "            **cache_opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b6371e5-cbe9-40b7-90c4-27885b453cbd",
   "metadata": {},
   "source": [
    "# Main loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28e420-6029-4384-b629-30d9efc196f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_patch(im, im_size_dva, patch_min_x_dva, patch_min_y_dva, wsize_dva, wsize_px, bgc=bgc):\n",
    "    assert isinstance(im, np.ndarray)# and im.shape[0] == im.shape[1]\n",
    "    assert isinstance(wsize_px, int)\n",
    "    map1 = np.arange(wsize_px)\n",
    "    map2 = map1.copy()\n",
    "    ppd = im.shape[0] / im_size_dva[0]\n",
    "    map1 = (\n",
    "        ppd * (map1+0.5) / wsize_px * wsize_dva\n",
    "        + ppd * (patch_min_x_dva + im_size_dva[0] / 2)\n",
    "    ).astype(np.float32)\n",
    "    ppd = im.shape[1] / im_size_dva[1]\n",
    "    map2 = (\n",
    "        ppd * (map2+0.5) / wsize_px * wsize_dva\n",
    "        + ppd * (-patch_min_y_dva -wsize_dva + im_size_dva[1] / 2)\n",
    "    ).astype(np.float32)\n",
    "    map1 = np.repeat(map1[None,:], wsize_px, 0)\n",
    "    map2 = np.repeat(map2[:,None], wsize_px, 1)\n",
    "    wim = cv2.remap(\n",
    "        im.astype(np.float32), map1, map2, interpolation=cv2.INTER_LINEAR,\n",
    "        borderMode=cv2.BORDER_CONSTANT, borderValue=bgc)\n",
    "    wim = np.round(wim).astype(np.uint8)\n",
    "    return wim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09056d21-0d4d-4b7c-b412-fc522ba09c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedder.pbar = iter  # to avoid bilayer tqdm\n",
    "for iim, (im, md5) in enumerate(zip(tqdm_(images), im_md5s)):\n",
    "    i_ = offset + iim\n",
    "\n",
    "    # full image feats\n",
    "    im_ = np.array(Image.fromarray(im).resize((model_imsize, model_imsize)))\n",
    "    feats_ = embedder.extract_pooled_features([im_])\n",
    "    feats = feats_[layer_name][0]\n",
    "\n",
    "    with h5.File(output_path, 'a') as f:\n",
    "        dset = f['feats/full_image']\n",
    "        if dset.shape[0] < i_ + 1:\n",
    "            dset.resize(i_+1, axis=0)\n",
    "        dset[i_] = feats\n",
    "\n",
    "    # patch grid feats\n",
    "    featss = []\n",
    "\n",
    "    for ix, x0 in enumerate(patches_ledge_x):\n",
    "        featss.append([])\n",
    "\n",
    "        for iy, y0 in enumerate(patches_ledge_y):\n",
    "            wim = get_image_patch(im, im_size, x0, y0, patch_size, model_imsize)\n",
    "            feats_ = embedder.extract_pooled_features([wim])\n",
    "            feats = feats_[layer_name][0]\n",
    "            featss[-1].append(feats)\n",
    "\n",
    "    featss = np.array(featss)\n",
    "\n",
    "    with h5.File(output_path, 'a') as f:\n",
    "        dset = f['feats/patch_grid']\n",
    "        if dset.shape[0] < i_ + 1:\n",
    "            dset.resize(i_+1, axis=0)\n",
    "        dset[i_] = featss\n",
    "\n",
    "        dset = f['md5']\n",
    "        if dset.shape[0] < i_ + 1:\n",
    "            dset.resize(i_+1, axis=0)\n",
    "        dset[i_] = md5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0471dc41-c4b7-4066-bd3f-5f49f5677356",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c09cf0-a0e4-45ae-9b50-00864abe86b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark -vm --iversions -rbg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
