{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b60b3877-02c0-45c5-8e8e-67622c63943a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../lib')\n",
    "from local_paths import preproc_dir, analysis_dir, database_dir\n",
    "from storage import get_storage_functions\n",
    "from hier_group import unpack_hier_names\n",
    "import roi_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87163e84-7739-45e5-8243-391bb5ddc47c",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5c9b1c-bb5c-437a-9e1a-1e0855327000",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# session\n",
    "#============================================================================\n",
    "sess_name = 'sess_name'\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# ROI-based categorization\n",
    "#============================================================================\n",
    "face_cats = ('face', 'primate_face')\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# Time windows\n",
    "#============================================================================\n",
    "# for FSI\n",
    "t_win = 150  # window width, starting from latency per-unit\n",
    "\n",
    "# time windows for PSTHs\n",
    "t_pre  = 375\n",
    "t_post = 375\n",
    "t_step =  10\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# statistics\n",
    "#============================================================================\n",
    "stat_tests_fsi = (('Fixation 1+', 'Fixation 0', 'two-sided'),)\n",
    "stat_tests_fix = (('Face', 'Non', 'greater'),)\n",
    "stat_tests_sacc = (\n",
    "    (('Non', 'Non'), ('Non', 'Face'), 'less'),\n",
    "    (('Face', 'Non'), ('Face', 'Face'), 'less'),\n",
    "    (('Non', 'Face'), ('Face', 'Face'), 'two-sided'),\n",
    "    (('Non', 'Non'), ('Face', 'Non'), 'two-sided'))\n",
    "n_perm = 10000\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# paths\n",
    "#============================================================================\n",
    "proc_dir = preproc_dir\n",
    "sdf_dir = preproc_dir\n",
    "sdf_suffix = '-mwa_50'\n",
    "rois_dir = '../db/ROIs'\n",
    "\n",
    "rf_fit_path = database_dir + 'per_unit_rf.csv.gz'\n",
    "latency_path = database_dir + 'per_unit_latency-fix_on.csv.gz'\n",
    "\n",
    "output_dir = analysis_dir + 'face_specific'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522483c-72b1-4b1b-8df1-d4a9b9df57a7",
   "metadata": {},
   "source": [
    "# Check parameters and whether already done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16f2001-6f92-47cc-bba0-c03214fb207b",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = Path(proc_dir) / (sess_name + '-proc.h5')\n",
    "print('Loading shared processing from', proc_path)\n",
    "proc_path = proc_path.expanduser()\n",
    "assert proc_path.is_file()\n",
    "\n",
    "rasters_path = Path(proc_dir) / (sess_name + '-rasters.nwb')\n",
    "print('Loading rasters from', rasters_path)\n",
    "rasters_path = rasters_path.expanduser()\n",
    "assert rasters_path.is_file()\n",
    "\n",
    "sdf_path = Path(sdf_dir) / (sess_name + f'-sdf{sdf_suffix}.h5')\n",
    "print('Loading spike density function from', sdf_path)\n",
    "sdf_path = sdf_path.expanduser()\n",
    "assert sdf_path.is_file()\n",
    "\n",
    "rf_fit_path = Path(rf_fit_path)\n",
    "print('Loading Gaussian-fitted RF maps density function from', rf_fit_path)\n",
    "rf_fit_path = rf_fit_path.expanduser()\n",
    "assert rf_fit_path.is_file()\n",
    "\n",
    "print('Using per-unit latency from', latency_path)\n",
    "latency_path = Path(latency_path).expanduser()\n",
    "assert latency_path.is_file()\n",
    "\n",
    "print('Using ROIs in', rois_dir)\n",
    "rois_dir = Path(rois_dir).expanduser()\n",
    "assert rois_dir.is_dir()\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "assert output_dir.expanduser().is_dir()\n",
    "output_path = output_dir / (sess_name + '.h5')\n",
    "print('Saving results to', output_path)\n",
    "output_path = output_path.expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90613d2f-e9cb-4823-a7a2-5641960dd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'face_specific'\n",
    "\n",
    "if output_path.is_file():\n",
    "    with h5.File(output_path, 'r') as f:\n",
    "        try:\n",
    "            if f[f'progress_report/{analysis_name}/all_done'][()].item():\n",
    "                raise RuntimeError(f'{sess_name} has already been processed')\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806f68e0-1f36-4918-b9d7-66d61b003d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(proc_path, 'r') as f:\n",
    "    random_seed = f['config/default_random_seed'][()]\n",
    "    im_root = f['stimulus/folder'][()].decode()  # scalar bytes -> str\n",
    "    im_subds = f['stimulus/subdirectories'][()].astype(str)  # np array bytes -> str\n",
    "im_dirs = set(f'{im_root}/{subd}'.strip('/') for subd in im_subds)\n",
    "\n",
    "# check ROIs are available\n",
    "print('Using ROIs in')\n",
    "bydir_rois_dir = {}\n",
    "for im_dir in set(im_dirs):\n",
    "    rois_dir_ = rois_dir / im_dir.replace('/', '__')\n",
    "    print(f'-\\t{im_dir+\":\":<20}\\t{rois_dir_}')\n",
    "\n",
    "    # check for ROIs\n",
    "    if rois_dir_.is_dir():\n",
    "        # check for face ROIs\n",
    "        with_face = False\n",
    "        for n in face_cats:\n",
    "            if (rois_dir_ / n).is_dir() or (rois_dir_ / f'{n}.csv').is_file():\n",
    "                with_face = True\n",
    "                break\n",
    "        if with_face:\n",
    "            bydir_rois_dir[im_dir] = rois_dir_\n",
    "        else:\n",
    "            print('\\tNo face ROIs available for', im_dir)\n",
    "\n",
    "    else:\n",
    "        print('\\tNo ROIs available for', im_dir)\n",
    "\n",
    "assert len(bydir_rois_dir), f'No face ROIs available for any of the {len(set(im_dirs))} image dirs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb704fe6-058e-43fb-97a4-7a4e711a3855",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results, add_attr_to_dset, check_equals_saved, link_dsets, copy_group = \\\n",
    "    get_storage_functions(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c26340-c80b-416a-8dcb-4266219379fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/config/'\n",
    "save_results(group+'face_cats', np.array(face_cats, dtype=bytes))\n",
    "save_results(group+'sdf_suffix', sdf_suffix)\n",
    "save_results(group+'random_seed', random_seed)\n",
    "\n",
    "group = analysis_name + '/config/time_windows/'\n",
    "save_results(group+'t_win', t_win)\n",
    "save_results(group+'t_pre',  t_pre)\n",
    "save_results(group+'t_post', t_post)\n",
    "save_results(group+'t_step', t_step)\n",
    "add_attr_to_dset(group, attrs=dict(unit='ms'))\n",
    "\n",
    "group = analysis_name + '/config/statistics/'\n",
    "save_results(group+'tests_fix', str(stat_tests_fix))\n",
    "save_results(group+'tests_sacc', str(stat_tests_sacc))\n",
    "save_results(group+'test_kind', 'mannwhitneyu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9963de-dae2-4c2a-b250-ca0f49497f27",
   "metadata": {},
   "source": [
    "# Define RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe2f026-aba7-4023-82af-49da4f16a160",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(sdf_path, 'r') as f:\n",
    "    unit_names = f['sdf'].attrs['unit_names'].astype(str)\n",
    "    if 'unit_names' in f:\n",
    "        copy_group(f, 'unit_names', analysis_name+'/unit_names')\n",
    "\n",
    "unit_names0 = unit_names.copy()\n",
    "unit_names = unpack_hier_names(unit_names)\n",
    "unit_names = unit_names[unit_names[:,0]=='Unit', 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825efc89-400a-47c8-9610-360f496d7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_rf_df = pd.read_csv(rf_fit_path).set_index('Session')\n",
    "assert sess_name in unit_rf_df.index, 'No unit has good RF fits'\n",
    "\n",
    "unit_rf_df = unit_rf_df.loc[[sess_name]].set_index('Name')\n",
    "assert not unit_rf_df.index.has_duplicates\n",
    "unit_names = unit_rf_df.index.intersection(unit_names).values\n",
    "print(f'{len(unit_names)} of {len(unit_names0)} ({(len(unit_names)/len(unit_names0))*100:.1f}%) units have fitted RFs')\n",
    "\n",
    "unit_rf_df = unit_rf_df.loc[unit_names].reset_index()\n",
    "assert len(unit_rf_df) == len(unit_names)\n",
    "print('Num units using RF fit from each source:')\n",
    "print('\\t' + '\\n\\t'.join(str(unit_rf_df.groupby('Source').count()['x']).split('\\n')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59299337-b146-4fb4-91a2-5539df07c458",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/unit_names', unit_names.astype(bytes))\n",
    "unit_rf_df.to_hdf(output_path, analysis_name+'/rf_per_unit', mode='a', format='table', complevel=9, complib='zlib')\n",
    "unit_rf_df['Index'] = np.arange(len(unit_rf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc0d61f-94e4-4f73-967d-8baf53195cc4",
   "metadata": {},
   "source": [
    "# Categorize fixations using ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e8b322-5a42-4c6a-95f4-f1536c2c7ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_df = pd.read_hdf(proc_path, 'fixation_dataframe', 'r')\n",
    "\n",
    "with h5.File(proc_path, 'r') as f:\n",
    "    fix_sel = f['fixation_selection/fixation_indices'][()]\n",
    "    ppd = f['stimulus/pixels_per_degree'][()]\n",
    "    im_size_px = f['stimulus/size_px'][()]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a56a27-36c1-49f8-be1d-ee7ad65044a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = fix_df.iloc[fix_sel].reset_index()\n",
    "\n",
    "# select trials with define ROIs\n",
    "tr_sel = set()\n",
    "for itr, row in df_.groupby('Trial').first().iterrows():\n",
    "    subd = row['Image subdir']\n",
    "    d = f'{im_root}/{subd}'.strip('/')\n",
    "    if d in bydir_rois_dir:\n",
    "        tr_sel.add(itr)\n",
    "\n",
    "# select fixations within selected trials\n",
    "fix_subsel = df_['Trial'].isin(tr_sel).values\n",
    "print(f'from {fix_subsel.size} fixations, selecting {fix_subsel.sum()} ({fix_subsel.mean()*100:.1f}%) in image folder(s) with face ROIs')\n",
    "assert fix_subsel.any()\n",
    "fix_sel = fix_sel[fix_subsel]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20143e7-13e0-4e5a-b48a-6fcd0278f88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = np.unique(np.concatenate([\n",
    "    roi_utils.tally_roi_cats(d)\n",
    "    for d in bydir_rois_dir.values()]))\n",
    "print('Defined ROI categories:', categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46a38ed-5f2a-47aa-91f4-30d95538fd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_fix_sel = np.full_like(fix_sel, -1)\n",
    "for ii, (itr, ifix) in enumerate(fix_df.index[fix_sel]):\n",
    "    try:\n",
    "        next_fix_sel[ii] = fix_df.index.get_loc((itr, ifix+1))\n",
    "    except KeyError:\n",
    "        pass\n",
    "m = next_fix_sel != -1\n",
    "print(f'{m.sum()} of {m.size} ({m.mean()*100:.1f}%) selected fixations have a next fixation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab89f43-e322-4f7b-a90d-2fac05cced73",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_dfs = []\n",
    "cls_dfs = []\n",
    "next_cat_dfs = []\n",
    "\n",
    "unit_groups = []\n",
    "\n",
    "for igroup, ((rf_x, rf_y, rf_size), df_) in enumerate(tqdm(unit_rf_df.groupby(['x', 'y', 'r']))):\n",
    "    unit_groups.append(df_['Index'].values)\n",
    "\n",
    "    dilate_px = int(round(ppd.mean() * rf_size / 2))\n",
    "    rf_xy_px = np.round(np.array([rf_x, rf_y]) * ppd).astype(int)\n",
    "\n",
    "    cat_df = {k: [] for k in categories}\n",
    "    cls_df = {k: [] for k in categories}\n",
    "    next_cat_df = {k: [] for k in categories}\n",
    "\n",
    "    for ii, (_, row) in enumerate(fix_df.iloc[fix_sel].iterrows()):\n",
    "        imfn = row['Image filename']\n",
    "\n",
    "        rel = row[['Relative X', 'Relative Y']].values.astype(float) + [rf_x, rf_y]\n",
    "        rel_px = np.round(rel * ppd).astype(int)\n",
    "\n",
    "        subd = row['Image subdir']\n",
    "        d = f'{im_root}/{subd}'.strip('/')\n",
    "        rois_dir = bydir_rois_dir[d]\n",
    "\n",
    "        for cat in categories:\n",
    "            is_cat, cls = roi_utils.is_on_roi(\n",
    "                imfn, im_size_px, rel_px,\n",
    "                cat, rois_dir, dilate_px)\n",
    "            cat_df[cat].append(is_cat)\n",
    "            cls_df[cat].append(cls)\n",
    "\n",
    "        ifix_next = next_fix_sel[ii]\n",
    "        if ifix_next == -1:\n",
    "            for cat in categories:\n",
    "                next_cat_df[cat].append(False)\n",
    "        else:\n",
    "            next_row = fix_df.iloc[ifix_next]\n",
    "            next_rel = next_row[['Relative X', 'Relative Y']].values.astype(float) + [rf_x, rf_y]\n",
    "            next_rel_px = np.round(next_rel * ppd).astype(int)\n",
    "            for cat in categories:\n",
    "                is_cat, cls = roi_utils.is_on_roi(\n",
    "                    imfn, im_size_px, next_rel_px,\n",
    "                    cat, rois_dir, dilate_px)\n",
    "                next_cat_df[cat].append(is_cat)\n",
    "\n",
    "    cat_df = pd.DataFrame(cat_df)\n",
    "    cls_df = pd.DataFrame(cls_df)\n",
    "    next_cat_df = pd.DataFrame(next_cat_df)\n",
    "    cat_df['Fixation index'] = cls_df['Fixation index'] = next_cat_df['Fixation index'] = fix_sel\n",
    "    cat_df['Group'] = cls_df['Group'] = next_cat_df['Group'] = igroup\n",
    "    cat_dfs.append(cat_df)\n",
    "    cls_dfs.append(cls_df)\n",
    "    next_cat_dfs.append(next_cat_df)\n",
    "\n",
    "cat_df = pd.concat(cat_dfs).set_index(['Group', 'Fixation index'])\n",
    "cls_df = pd.concat(cls_dfs).set_index(['Group', 'Fixation index'])\n",
    "next_cat_df = pd.concat(next_cat_dfs).set_index(['Group', 'Fixation index'])\n",
    "\n",
    "# keep only categories that appear\n",
    "categories = list(cat_df.columns[cat_df.any(axis=0)])\n",
    "cat_df = cat_df[categories]\n",
    "cls_df = cls_df[categories]\n",
    "next_cat_df = next_cat_df[categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15231cb7-c6b7-4495-ac13-0666b9dd92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Overall fraction of fixations per category:')\n",
    "cat_df.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f83174f-be76-4096-a39f-8a545b80d364",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Num fixations per cluster per category (0 is backgroud)')\n",
    "for c in cls_df.columns:\n",
    "    print(cls_df[c].value_counts().sort_index())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4fa73e-0064-4a08-8a22-33a4c9cb6f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cats = list(set(cat_df.columns) & set(face_cats))\n",
    "fix_is_face = cat_df[face_cats].values.any(1)\n",
    "fix_is_non = ~cat_df.values.any(1)\n",
    "print(f'Fraction of face fixations: {fix_is_face.mean()*100:4.1f}%')\n",
    "print(f'Fraction of nonface fixations: {fix_is_non.mean()*100:4.1f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110c3cf2-a9e2-4477-8164-1ce09fc35862",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/'\n",
    "save_results(group+'fixation_selection', fix_sel)\n",
    "for igroup, usel in enumerate(unit_groups):\n",
    "    save_results(group+f'unit_groups/{igroup}', usel)\n",
    "cat_df.reset_index().to_hdf(output_path, group+'categorization', mode='a', format='table', complevel=9, complib='zlib')\n",
    "cls_df.reset_index().to_hdf(output_path, group+'cluster_id', mode='a', format='table', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95a9d2b-746c-4eac-ab46-4a7a31e6dc08",
   "metadata": {},
   "source": [
    "# Load neural data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75279c88-f982-465b-93c2-2422a8b1d272",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(sdf_path, 'r') as f:\n",
    "    dset = f['sdf']\n",
    "    all_unit_names = list(dset.attrs['unit_names'].astype(str))\n",
    "    sel_ = np.array([all_unit_names.index(n) for n in unit_names])\n",
    "    sdf = dset[()][:,sel_]\n",
    "\n",
    "sdf.shape, sdf.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfbbeb20-c810-4610-8dd9-8e68fc0311db",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(rasters_path, 'r') as f:\n",
    "    all_unit_names = list(f['processing/ecephys/unit_names/unit_name'][()].astype(str))\n",
    "    sel_ = np.array([all_unit_names.index(n) for n in unit_names])\n",
    "    rasters = f['processing/ecephys/rasters/data'][()][:,sel_]\n",
    "\n",
    "rasters.shape, rasters.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c82ad62-f19f-4ac5-adb0-24f9f36ab08c",
   "metadata": {},
   "source": [
    "# Get latency per unit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31720a3d-9e4d-4ac1-afb8-c49ed05c71ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_df = pd.read_csv(latency_path).set_index('Session').loc[[sess_name]].set_index('Name')\n",
    "m = pd.Series(unit_names).isin(lat_df.index)\n",
    "assert m.all(), f'missing latency value for {(~m).sum()} of {m.size} units'\n",
    "assert not lat_df.index.has_duplicates\n",
    "\n",
    "lat_df = lat_df.loc[unit_names].reset_index()\n",
    "assert len(lat_df) == len(unit_names)\n",
    "lat_df['Latency'] = np.clip(lat_df['Latency'], 40, None)\n",
    "print('Num units using RF fit from each source:')\n",
    "print('\\t' + '\\n\\t'.join(str(lat_df.groupby('Source').count()['Latency']).split('\\n')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a8bac7-7107-4c1a-9f1d-1798ecd9a7e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_df.to_hdf(output_path, analysis_name+'/latency_per_unit', mode='a', format='table', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c30c2d4b-6db7-4257-85bb-3242468f6421",
   "metadata": {},
   "source": [
    "# One-fixation analysis: FSI, PSTHs, Fix 0 vs Fix 1+\n",
    "Excluding face → face saccades for fix 1+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a540edd-45ac-46dd-9a8b-e466aa9faf6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = np.random.default_rng(random_seed)\n",
    "\n",
    "def calculate_fsi(a, b):\n",
    "    \"\"\" a, b shape (n_fix, n_neur) \"\"\"\n",
    "    # assert a.ndim == b.ndim == 2\n",
    "    # assert a.shape[1] == b.shape[1]\n",
    "    fsi = np.full(a.shape[1], np.nan, dtype=np.float32)\n",
    "    a = a.mean(0)\n",
    "    b = b.mean(0)\n",
    "    denom = a + b\n",
    "    m = denom != 0\n",
    "    fsi[m] = (a[m] - b[m]) / denom[m]\n",
    "    return fsi\n",
    "\n",
    "def fsi_perm_test(val_dict, test, alts, n_permutation=n_perm, range_=range, rg=rg):\n",
    "    valss = [\n",
    "        np.concatenate([val_dict[t][k] for t in test[:2]], axis=0)\n",
    "        for k in ('Face', 'Non')]\n",
    "    splits = []\n",
    "    for k in ('Face', 'Non'):\n",
    "        i = len(val_dict[test[0]][k])\n",
    "        splits.append((slice(None, i), slice(i, None)))\n",
    "\n",
    "    h1 = -1 * np.diff([\n",
    "        calculate_fsi(*(valss[j][splits[j][i]] for j in range(2)))\n",
    "        for i in range(2)], axis=0)\n",
    "    assert len(h1) == 1\n",
    "    h1 = h1[0]\n",
    "\n",
    "    h0s = np.empty_like(h1, shape=(n_permutation, *h1.shape))\n",
    "    for i in range_(n_permutation):\n",
    "        [rg.shuffle(v, axis=0) for v in valss]\n",
    "        h0s[i] = np.diff([\n",
    "            calculate_fsi(*(valss[j][splits[j][i]] for j in range(2)))\n",
    "            for i in range(2)], axis=0)\n",
    "\n",
    "    h1 = np.ma.masked_invalid(h1)\n",
    "    h0s = np.ma.masked_invalid(h0s)\n",
    "    pvalss = []\n",
    "    for alternative in alts:\n",
    "        if alternative == 'greater':\n",
    "            pvals = h0s >= h1\n",
    "        elif alternative == 'less':\n",
    "            pvals = h0s <= h1\n",
    "        else:\n",
    "            pvals = np.abs(h0s) >= np.abs(h1)\n",
    "\n",
    "        isvalid = np.broadcast_to(~pvals.mask, pvals.shape)\n",
    "        pvals = pvals.mean(0).filled(np.nan)\n",
    "        pvals_floor = 1/(isvalid.sum(0)+1)\n",
    "        pvals = np.clip(pvals, pvals_floor, None)\n",
    "\n",
    "        pvalss.append(pvals)\n",
    "\n",
    "    return pvalss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fd3ed5-ca4c-4145-8927-66e83a77ab75",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neur = unit_names.size\n",
    "ts = np.arange(-t_pre, t_post+t_step/10, t_step)\n",
    "\n",
    "f01_fn_nfix = np.empty((2, 2, n_neur), dtype=int)\n",
    "f01_fsi = np.full((2, n_neur), np.nan, dtype=np.float32)\n",
    "f01_fs_pvals = np.full((len(stat_tests_fix), 2, n_neur), np.nan)\n",
    "f0v1_fsi_pvals = np.full((len(stat_tests_fsi), n_neur), np.nan)\n",
    "f01_fn_psths_mean_std = np.empty((2, 2, 2, ts.size, n_neur), dtype=sdf.dtype)\n",
    "f01_psth_pvals = np.full((len(stat_tests_fix), 2, ts.size, n_neur), np.nan)\n",
    "\n",
    "stat_tests_fsi_df = pd.DataFrame(\n",
    "    data=stat_tests_fsi, columns=['Cond 0', 'Cond 1', 'Alt'])\n",
    "\n",
    "f01s = fix_df.iloc[fix_sel].index.get_level_values(fix_df.index.names.index('Fixation'))\n",
    "for igroup, usel in enumerate(unit_groups):\n",
    "    cat_df_ = cat_df.loc[igroup, slice(None)]\n",
    "    fix_is_non = ~cat_df_.values.any(1)\n",
    "    fix_is_face = cat_df_[face_cats].values.any(1)\n",
    "\n",
    "    next_cat_df_ = next_cat_df.loc[igroup, slice(None)]\n",
    "    next_fix_is_face = next_cat_df_[face_cats].values.any(1)\n",
    "    fix_is_face = fix_is_face & ~next_fix_is_face\n",
    "\n",
    "    lat_df_ = lat_df.iloc[usel].copy()\n",
    "    lat_df_['Index'] = np.arange(len(lat_df_))\n",
    "    lat_groups_ = [(dt, df_['Index'].values) for dt, df_ in lat_df_.groupby('Latency')]\n",
    "\n",
    "    byf01_fn_resps = {}\n",
    "\n",
    "    for f01 in range(2):\n",
    "        t_win_ = np.array([0, t_win])\n",
    "        fn_resps = {}\n",
    "        fn_psths = {}\n",
    "        for icat, (cat, fix_is_cat) in enumerate((\n",
    "                ('Non', fix_is_non),\n",
    "                ('Face', fix_is_face))):\n",
    "            if f01 == 0:\n",
    "                m = f01s == 0 & fix_is_cat\n",
    "            else:\n",
    "                m = f01s > 0\n",
    "            m &= fix_is_cat\n",
    "            fix_sel_ = fix_sel[m]\n",
    "            nfix = fix_sel_.size\n",
    "\n",
    "            resps = np.empty((nfix, usel.size), dtype=np.float32)\n",
    "            psths = np.empty((nfix, ts.size, usel.size), dtype=sdf.dtype)\n",
    "\n",
    "            for i, t in enumerate(fix_df.iloc[fix_sel_]['Time'].values):\n",
    "                for dt, usubsel in lat_groups_:\n",
    "                    s = slice(*np.round(t+dt+t_win_).astype(int))\n",
    "                    resps[i,usubsel] = rasters[s, usel[usubsel]].mean(0)\n",
    "\n",
    "                ts_ = np.round(t+ts).astype(int)\n",
    "                psths[i] = sdf[ts_][:, usel]\n",
    "\n",
    "            fn_resps[cat] = resps\n",
    "            fn_psths[cat] = psths\n",
    "            f01_fn_nfix[f01, icat, usel] = nfix\n",
    "            f01_fn_psths_mean_std[f01,icat,0,:,usel] = psths.mean(0).T\n",
    "            f01_fn_psths_mean_std[f01,icat,1,:,usel] = psths.std(0).T\n",
    "\n",
    "        byf01_fn_resps[('Fixation 0', 'Fixation 1+')[f01]] = fn_resps\n",
    "        f01_fsi[f01, usel] = calculate_fsi(fn_resps['Face'], fn_resps['Non'])\n",
    "\n",
    "        for it, test in enumerate(stat_tests_fix):\n",
    "            alt = test[2] if len(test) > 2 else 'two-sided'\n",
    "            x, y = (fn_resps[cond] for cond in test[:2])\n",
    "            if x.size and y.size:\n",
    "                f01_fs_pvals[it,f01,usel] = st.mannwhitneyu(\n",
    "                    x=x, y=y, alternative=alt, axis=0).pvalue\n",
    "            x, y = (fn_psths[cond] for cond in test[:2])\n",
    "            if x.size and y.size:\n",
    "                f01_psth_pvals[it,f01,:,usel] = st.mannwhitneyu(\n",
    "                    x=x, y=y, alternative=alt, axis=0).pvalue.T\n",
    "\n",
    "    for test, df_ in stat_tests_fsi_df.groupby(['Cond 0', 'Cond 1']):\n",
    "        alts = list(df_['Alt'].values)\n",
    "        pvalss = fsi_perm_test(byf01_fn_resps, test, alts)\n",
    "        for it, pvals in zip(df_.index.values, pvalss):\n",
    "            f0v1_fsi_pvals[it,usel] = pvals\n",
    "\n",
    "del resps, psths, fn_resps, fn_psths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b7c7a9-841c-4c57-bf5e-6b9ea929060f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = np.array(('test', 'two_sample_test', 'fixation_type', 'category', 'statistic', 'time', 'unit'))\n",
    "compr = dict(compression='gzip', compression_opts=9)\n",
    "coords = {\n",
    "    'test': np.array([str(v) for v in stat_tests_fix]),\n",
    "    'two_sample_test': np.array([str(v) for v in stat_tests_fsi]),\n",
    "    'fixation_type': np.array(('zeroth', 'non-zeroth')),\n",
    "    'category': np.array(('Non', 'Face')),\n",
    "    'statistic': np.array(('Non', 'Face')),\n",
    "    'time': ts,\n",
    "    'unit': unit_names}\n",
    "group = analysis_name + '/by_fixation/'\n",
    "\n",
    "\n",
    "attrs = {'t_win': t_win, 'n_permutations': n_perm}\n",
    "data_vars = {\n",
    "    'face_selectivity_index': (dims[[2,-1]], f01_fsi),\n",
    "    'p-value': (dims[[0,2,-1]], f01_fs_pvals),\n",
    "    'two-sample_p-value': (dims[[1,-1]], f0v1_fsi_pvals),\n",
    "    'num_fixations': (dims[[2,3,-1]], f01_fn_nfix)}\n",
    "dataset = xr.Dataset(data_vars, coords=coords, attrs=attrs)\n",
    "encoding = {\n",
    "    k: dict(chunksizes=v.shape, **compr) if v.size else {}\n",
    "    for k, v in dataset.data_vars.items()}\n",
    "dataset.to_netcdf(\n",
    "    output_path, group=group+'face_selectivity',\n",
    "    mode='a', engine='h5netcdf', encoding=encoding)\n",
    "\n",
    "\n",
    "data_vars = {\n",
    "    'data': (dims[2:], f01_fn_psths_mean_std),\n",
    "    'p-value': (dims[[0,2,-2,-1]], f01_psth_pvals),\n",
    "    'num_fixations': (dims[[2,3,-1]], f01_fn_nfix)}\n",
    "dataset = xr.Dataset(data_vars, coords=coords)\n",
    "dataset['data'].attrs.update({'unit': 'spikes/s'})\n",
    "encoding = {\n",
    "    k: dict(chunksizes=v.shape, **compr) if v.size else {}\n",
    "    for k, v in dataset.data_vars.items()}\n",
    "dataset.to_netcdf(\n",
    "    output_path, group=group+'psth',\n",
    "    mode='a', engine='h5netcdf', encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a140252-0910-40b9-950d-918d2c621950",
   "metadata": {},
   "source": [
    "# Across-saccade analysis: PSTHs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6855b52-4e4c-4ba4-a704-f685f110410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(proc_path, 'r') as f:\n",
    "    sacc_sel = f['saccade_selection/fixation_indices'][()]  # shape (2, n)\n",
    "    lg_sacc_mask = f['saccade_selection/large/saccade_subset'][()]  # shape (n,) bool mask\n",
    "\n",
    "# to avoid onset-transient, sub-select saccades that do not start_with fix 0\n",
    "non_zeroth = 0 != fix_df.index.get_level_values(fix_df.index.names.index('Fixation'))[sacc_sel[0]]\n",
    "print(f'from {sacc_sel.shape[1]} saccades, selecting:')\n",
    "print(f'\\t{non_zeroth.sum()} ({non_zeroth.mean()*100:.1f}%) to exclude zeroth fixations')\n",
    "\n",
    "# select saccades within selected trials\n",
    "df_ = fix_df.iloc[sacc_sel[0]].reset_index()\n",
    "sacc_subsel = df_['Trial'].isin(tr_sel).values\n",
    "df_ = fix_df.iloc[sacc_sel[1]].reset_index()\n",
    "sacc_subsel &= df_['Trial'].isin(tr_sel).values\n",
    "print(f'\\t{sacc_subsel.sum()} ({sacc_subsel.mean()*100:.1f}%) in image folder(s) with face ROIs')\n",
    "\n",
    "sacc_subsel &= non_zeroth\n",
    "print(f'\\t{sacc_subsel.sum()} ({sacc_subsel.mean()*100:.1f}%) by both two criteria1')\n",
    "sacc_sel = sacc_sel[:,sacc_subsel]\n",
    "lg_sacc_mask = lg_sacc_mask[sacc_subsel]\n",
    "\n",
    "assert not(set(sacc_sel.ravel()) - set(fix_sel) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53991228-1735-41d1-b073-820ca1b1d211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide saccades into 2x2 categories\n",
    "sacc_cat = np.full_like(sacc_sel, -1, dtype=int)\n",
    "fix_is_face_ = cat_df[face_cats].groupby('Fixation index').any().any(axis=1)\n",
    "fix_is_non_ = ~cat_df.any(axis=1).groupby('Fixation index').any()\n",
    "for i in range(2):\n",
    "    sacc_cat[i, fix_is_non_.loc[sacc_sel[i]]] = 0\n",
    "    sacc_cat[i, fix_is_face_.loc[sacc_sel[i]]] = 1\n",
    "m = (sacc_cat >= 0).all(0)\n",
    "print(f'from {m.size} saccades ({lg_sacc_mask.sum()} large),', end=' ')\n",
    "sacc_sel = sacc_sel[:,m]\n",
    "lg_sacc_mask = lg_sacc_mask[m]\n",
    "sacc_cat = sacc_cat[:,m]\n",
    "print(f'selecting {m.sum()} ({lg_sacc_mask.sum()} large) between face and nonface')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e8e195-d4e7-4ad1-b2b4-8a61068b8d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equalize images across saccade selections:\n",
    "#    further select those saccades occurring on\n",
    "#    images that contained any condition involving face fixations\n",
    "#    this makes sure non -> non saccades concern the same images involved in\n",
    "#    (any of) the other 3 conditions. the criteria below consider both fixations\n",
    "#    across a saccade, because if, say, only fix 2 is considered, a saccade such\n",
    "#    as 3rd-category -> face would lead to the inclusion of an image that may not\n",
    "#    be involved in any of the 2x2 conditions\n",
    "imfns_sel = fix_df.iloc[sacc_sel[0, (sacc_cat>0).any(0) & lg_sacc_mask]]['Image filename'].unique()\n",
    "\n",
    "print(f'from {sacc_sel.shape[1]} saccades ({lg_sacc_mask.sum()} large),', end=' ')\n",
    "m = fix_df.iloc[sacc_sel[0]]['Image filename'].isin(imfns_sel)\n",
    "sacc_sel = sacc_sel[:,m]\n",
    "lg_sacc_mask = lg_sacc_mask[m]\n",
    "sacc_cat = sacc_cat[:,m]\n",
    "print(f'selecting {m.sum()} ({lg_sacc_mask.sum()} large) '\n",
    "      f'involving {len(imfns_sel)} images with face saccades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b101f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "byscond_scat_psth_mean_std = {\n",
    "    # fix 1 cat, fix 2 cat, mean/std, time, unit\n",
    "    k: np.full((2, 2, 2, ts.size, n_neur), np.nan, dtype=sdf.dtype)\n",
    "    for k in ('all', 'large')}\n",
    "byscond_sacc_cat_dfs = {k: [] for k in ('all', 'large')}\n",
    "byscond_pvals = {\n",
    "    k: np.full((len(stat_tests_sacc), ts.size, n_neur), np.nan)\n",
    "    for k in ('all', 'large')}\n",
    "\n",
    "for igroup, usel in enumerate(tqdm(unit_groups)):\n",
    "    # divide saccades into 2x2 categories\n",
    "    sacc_cat_ = np.full_like(sacc_sel, -1, dtype=int)\n",
    "    fix_is_face_ = cat_df.loc[(igroup,slice(None)),][face_cats].any(axis=1)\n",
    "    fix_is_non_ = ~cat_df.loc[(igroup,slice(None)),].any(axis=1)\n",
    "    for i in range(2):\n",
    "        sacc_cat_[i, fix_is_non_.loc[(igroup,sacc_sel[i]),]] = 0\n",
    "        sacc_cat_[i, fix_is_face_.loc[(igroup,sacc_sel[i]),]] = 1\n",
    "    m = (sacc_cat_ >= 0).all(0)\n",
    "    sacc_sel_ = sacc_sel[:,m]\n",
    "    lg_sacc_mask_ = lg_sacc_mask[m]\n",
    "    sacc_cat_ = sacc_cat_[:,m]\n",
    "\n",
    "    sacc_cat_df = pd.DataFrame(np.array(['Non', 'Face'])[sacc_cat_.T], columns=['Fix 1 cat', 'Fix 2 cat'])\n",
    "    sacc_cat_df[['Fix 1 index', 'Fix 2 index']] = sacc_sel_.T\n",
    "\n",
    "    byscat_psths = {}\n",
    "    for scond, sacc_cat_df_ in (\n",
    "            ('all', sacc_cat_df),\n",
    "            ('large', sacc_cat_df[lg_sacc_mask_])):\n",
    "        psth_mean_std = byscond_scat_psth_mean_std[scond]\n",
    "\n",
    "        for scat, df_ in sacc_cat_df_.groupby(['Fix 1 cat', 'Fix 2 cat']):\n",
    "            i1, i2 = map(['Non', 'Face'].index, scat)\n",
    "            n_sacc = len(df_)\n",
    "            psths = np.empty((n_sacc, ts.size, usel.size), dtype=sdf.dtype)\n",
    "            for i, t in enumerate(fix_df.iloc[df_['Fix 2 index'].values]['Time'].values):\n",
    "                ts_ = np.round(t+ts).astype(int)\n",
    "                psths[i] = sdf[ts_][:, usel]\n",
    "            byscat_psths[scat] = psths\n",
    "            psth_mean_std[i1,i2,0,:,usel] = psths.mean(0).T  # transpose due to\n",
    "            psth_mean_std[i1,i2,1,:,usel] = psths.std(0).T   # numpy indexing weirdness\n",
    "\n",
    "        sacc_cat_df_ = sacc_cat_df_.copy()\n",
    "        sacc_cat_df_['Group'] = igroup\n",
    "        byscond_sacc_cat_dfs[scond].append(sacc_cat_df_)\n",
    "\n",
    "        for it, test in enumerate(stat_tests_sacc):\n",
    "            c0, c1 = test[:2]\n",
    "            alt = test[2] if len(test) > 2 else 'two-sided'\n",
    "            if c0 in byscat_psths and c1 in byscat_psths:\n",
    "                byscond_pvals[scond][it,:,usel] = st.mannwhitneyu(\n",
    "                    x=byscat_psths[c0],\n",
    "                    y=byscat_psths[c1],\n",
    "                    alternative=alt,\n",
    "                    axis=0\n",
    "                ).pvalue.T  # transpose due to numpy indexing weirdness\n",
    "\n",
    "del sdf\n",
    "\n",
    "for k, v in byscond_sacc_cat_dfs.items():\n",
    "    byscond_sacc_cat_dfs[k] = pd.concat(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5d2cfd-edcc-4190-9bd4-7299dc8461a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = np.array(('test', 'fix1_category', 'fix2_category', 'statistic', 'time', 'unit'))\n",
    "coords = {\n",
    "    'test': np.array([str(v) for v in stat_tests_sacc]),\n",
    "    'fix1_category': np.array(('Non', 'Face')),\n",
    "    'fix2_category': np.array(('Non', 'Face')),\n",
    "    'statistic': np.array(['mean', 'std']),\n",
    "    'time': ts,\n",
    "    'unit': unit_names}\n",
    "compr = dict(compression='gzip', compression_opts=9)\n",
    "\n",
    "for scond, psth_mean_std in byscond_scat_psth_mean_std.items():\n",
    "    group = analysis_name + f'/by_saccade/by_saccade_size/{scond}/'\n",
    "    pvals = byscond_pvals[scond]\n",
    "    data_vars = {\n",
    "        'data': (dims[1:], psth_mean_std),\n",
    "        'p-value': (dims[[0,-2,-1]], pvals)}\n",
    "    dataset = xr.Dataset(data_vars, coords=coords)\n",
    "    dataset['data'].attrs.update({'unit': 'spikes/s'})\n",
    "\n",
    "    encoding = {\n",
    "        k: dict(chunksizes=v.shape, **compr) if v.size else {}\n",
    "        for k, v in dataset.data_vars.items()}\n",
    "    dataset.to_netcdf(\n",
    "        output_path, group=group+'psth',\n",
    "        mode='a', engine='h5netcdf', encoding=encoding)\n",
    "\n",
    "    kwargs = dict(mode='a', format='table', complevel=9, complib='zlib')\n",
    "    byscond_sacc_cat_dfs[scond]\\\n",
    "        .to_hdf(output_path, group+'saccade_categorization', **kwargs)\n",
    "    byscond_sacc_cat_dfs[scond].groupby(['Group', 'Fix 1 cat', 'Fix 2 cat'])\\\n",
    "        .count()['Fix 2 index'].rename('Count').reset_index()\\\n",
    "        .to_hdf(output_path, group+'num_saccades', **kwargs)\n",
    "\n",
    "    if scond == 'large':\n",
    "        save_results(group+'saccade_subset', lg_sacc_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74ad216-13f2-461a-af8d-ce940ef0ef18",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50b16a-744e-4328-9cbc-5813d8baed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(f'progress_report/{analysis_name}/all_done', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e15c0-cf86-4e73-860d-b55bafb4bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark -vm --iversions -rbg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42a85c9-937e-4470-9f4e-c37cbff4490e",
   "metadata": {},
   "source": [
    "# Basic visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e848d575-0fd0-4f98-8f4a-96ca0aeab145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05987b6d-ff78-4f1b-8385-3dc1371d30a5",
   "metadata": {},
   "source": [
    "### FSI for fix 0 and fix 1+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6231f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if f01_fsi.size and np.isfinite(f01_fsi).any(1).all(0):\n",
    "    x = 'FSI, fixation 0'\n",
    "    y = 'FSI, fixation 1+'\n",
    "    h = 'Sig. selective'\n",
    "    data = pd.DataFrame(data={x: f01_fsi[0], y: f01_fsi[1], h: list(map(tuple, (f01_fs_pvals[0]<0.01).T))})\n",
    "    jg = sns.jointplot(data=data, x=x, y=y, hue=h)\n",
    "    b = np.ma.masked_invalid(f01_fsi)\n",
    "    b = [b.min(1).max(0), b.max(1).min(0)]\n",
    "    jg.ax_joint.plot(b, b, color='gray', ls='--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82671036-7a1e-4d93-946a-dac7bfcbc7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if f01_fsi.size:\n",
    "    b = np.ma.masked_invalid(f01_fsi)\n",
    "    m = ~b.mask.any(0)\n",
    "    if m.any():\n",
    "        b = b[:,m]\n",
    "        x = 'FSI, fixation 0'\n",
    "        y = 'FSI, fixation 1+'\n",
    "        h = 'Sig. different'\n",
    "        data = pd.DataFrame(data={x: f01_fsi[0], y: f01_fsi[1], h: list(map(tuple, (f0v1_fsi_pvals<0.01).T))})\n",
    "        data = data[m]\n",
    "        jg = sns.jointplot(data=data, x=x, y=y, hue=h)\n",
    "        b = [b.min(1).max(0), b.max(1).min(0)]\n",
    "        jg.ax_joint.plot(b, b, color='gray', ls='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b371b7-efd5-4731-8301-c77a403d294c",
   "metadata": {},
   "source": [
    "### Face vs. non-face PSTHs for fix 0 and fix 1+"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d649607e-ba5c-47c8-a3c6-7d2af6a26152",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(8,2.5))\n",
    "for f01, ax in enumerate(axs):\n",
    "    for icat, cat in enumerate(('Non', 'Face')):\n",
    "        r = f01_fn_psths_mean_std[f01, icat, 0].mean(1)\n",
    "        ax.plot(ts, r, label=cat)\n",
    "    ax.set_title('Fixation ' + ('0', '1+')[f01])\n",
    "    ax.set_xlabel('Time rel. fix. on, ms')\n",
    "ax.legend(title='Category', loc='upper left', bbox_to_anchor=(1,1.05))\n",
    "axs[0].set_ylabel('Grand mean FR, spks/s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c73229-2911-4ac6-8202-c2ab5be6ad76",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(stat_tests_fix)\n",
    "fig, axs = plt.subplots(n, 2, figsize=(8,2.5*n), squeeze=False)\n",
    "for it, test in enumerate(stat_tests_fix):\n",
    "    axs_ = axs[it]\n",
    "    for f01, ax in enumerate(axs_):\n",
    "        vs = f01_psth_pvals[0][f01]\n",
    "        ax.plot(ts, vs, c='k', lw=0.5)\n",
    "        ax.set_yscale('log')\n",
    "        ax.hlines(.01, ts.min(), ts.max(), ls='--', lw=0.5, color='r')\n",
    "        ax.set_title('Fixation ' + ('0', '1+')[f01])\n",
    "    ax = axs_[0]\n",
    "    ax.text(.05, .05, f'{test[0]} {dict(less=\"<\",greater=\">\").get(test[2],\"!=\")} {test[1]}',\n",
    "            ha='left', va='bottom', transform=ax.transAxes, fontsize=8)\n",
    "    ax.set_xlabel('Time rel. fix. on, ms')\n",
    "    ax.set_ylabel('P-value');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98044c5-9529-4072-b652-5a4ba4bfd9f6",
   "metadata": {},
   "source": [
    "### Two-category PSTHs across saccades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b484c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scond, scat_psth_mean_std in byscond_scat_psth_mean_std.items():\n",
    "    if not scat_psth_mean_std.size: continue\n",
    "    plt.figure()\n",
    "    for icat1, cat1 in enumerate(('Non', 'Face')):\n",
    "        for icat2, cat2 in enumerate(('Non', 'Face')):\n",
    "            r = np.nanmean(scat_psth_mean_std[icat1, icat2, 0], axis=1)\n",
    "            plt.plot(ts, r, label=cat1+r'$\\rightarrow$'+cat2)\n",
    "    plt.xlabel('Time rel. fix. on, ms')\n",
    "    plt.ylabel('Grand mean FR, spks/s')\n",
    "    plt.legend(title='Saccade category', loc='upper left', bbox_to_anchor=(1,1.05))\n",
    "    plt.title(scond.capitalize() + ' saccades')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2720b28-935e-44d9-8f0f-e334a33a66a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for scond, pvals in byscond_pvals.items():\n",
    "    if not pvals.size: continue\n",
    "    n = len(pvals)\n",
    "    fig, axs = plt.subplots(1, n, figsize=(4*n,3), squeeze=False)\n",
    "    plt.subplots_adjust(wspace=0.3)\n",
    "    axs = axs.ravel()\n",
    "    f = lambda x: x if isinstance(x, str) else ', '.join(map(str, x))\n",
    "    for i, ax in enumerate(axs):\n",
    "        vs = pvals[i]\n",
    "        ax.plot(ts, vs, c='k', lw=0.5);\n",
    "        ax.set_yscale('log')\n",
    "        ax.hlines(.01, ts.min(), ts.max(), ls='--', lw=0.5, color='r')\n",
    "        test = stat_tests_sacc[i]\n",
    "        if len(test) < 3:\n",
    "            test = test + ('two-sided',)\n",
    "        ax.text(.05, .05, f'{f(test[0])} {dict(less=\"<\",greater=\">\").get(test[2],\"!=\")}\\n{f(test[1])}',\n",
    "                ha='left', va='bottom', transform=ax.transAxes, fontsize=8)\n",
    "    fig.suptitle(scond.capitalize() + ' saccades')\n",
    "    axs[0].set_xlabel('Time rel. fix. on, ms')\n",
    "    axs[0].set_ylabel('P-value')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
