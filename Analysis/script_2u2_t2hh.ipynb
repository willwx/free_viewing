{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59898f1-e75d-42a9-8996-a4339d5f5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../lib')\n",
    "from local_paths import analysis_dir, annot_path\n",
    "from storage import get_storage_functions\n",
    "from hier_group import unpack_hier_names, annot_names, hier_lookup\n",
    "import crossing_point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ee39f1-8a78-44d2-a718-f1834450ebc5",
   "metadata": {},
   "source": [
    "# Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19f5243e-a7db-451e-90c2-f5608a9fc83b",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# session\n",
    "#============================================================================\n",
    "sess_name = 'sess_name'\n",
    "\n",
    "#============================================================================\n",
    "# config\n",
    "#============================================================================\n",
    "input_analysis_name = 'fix0_self_consistency'\n",
    "conds = 'return_fixation,same_image'\n",
    "clearance_thres = 100\n",
    "\n",
    "#============================================================================\n",
    "# paths\n",
    "#============================================================================\n",
    "rsc_dir = analysis_dir + 'fix0_self_consistency'\n",
    "output_dir = rsc_dir + '-t2hh'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e6ac86-dc81-4db2-a2a9-77656e53d52b",
   "metadata": {},
   "source": [
    "# Check prereqs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c700cb3-ae5b-4a23-baaa-70eeee9e8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "rsc_path = Path(rsc_dir) / (sess_name + '.h5')\n",
    "print('Loading return-fixation self-consistency results from', rsc_path)\n",
    "rsc_path = rsc_path.expanduser()\n",
    "assert rsc_path.is_file()\n",
    "\n",
    "print('Using recording location annotations at', annot_path)\n",
    "annot_path = Path(annot_path).expanduser()\n",
    "assert annot_path.is_file()\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "assert output_dir.expanduser().is_dir()\n",
    "output_path = output_dir / (sess_name + '.h5')\n",
    "print('Saving results to', output_path)\n",
    "output_path = output_path.expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3212dade-53c5-4c46-a638-caee8344887c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(rsc_path, 'r') as f:\n",
    "    sdf_suffix = f[input_analysis_name+'/config/sdf_suffix'][()].decode()\n",
    "    t_step = f[input_analysis_name+'/config/time_windows/t_step'][()]\n",
    "sdf_suffix, t_step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90613d2f-e9cb-4823-a7a2-5641960dd636",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'time_to_half_height'\n",
    "\n",
    "if output_path.is_file():\n",
    "    with h5.File(output_path, 'r') as f:\n",
    "        try:\n",
    "            if f[f'progress_report/{analysis_name}/all_done'][()].item():\n",
    "                raise RuntimeError(f'{sess_name} has already been processed')\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6d9e29-f2c5-46bb-abec-41d6b05876a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results, add_attr_to_dset, check_equals_saved, link_dsets, copy_group = \\\n",
    "    get_storage_functions(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa9c6feb-7e5a-40be-9da5-bb6526f36722",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/config/'\n",
    "save_results(group+'input_analysis_name', input_analysis_name)\n",
    "save_results(group+'conds', conds)\n",
    "save_results(group+'clearance_thres', clearance_thres, attrs=dict(unit='ms'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e65a19b-be71-47f3-8112-631357610eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "conds = conds.split(',')\n",
    "print('conditions to be processed in parallel:', conds)\n",
    "\n",
    "conds_ = []\n",
    "with h5.File(rsc_path, 'r') as f:\n",
    "    for cond in conds:\n",
    "        obj = f[input_analysis_name + '/' + cond]\n",
    "        print(cond, obj)\n",
    "        if isinstance(obj, h5.Dataset) and not (obj.size):\n",
    "            continue\n",
    "        conds_.append(cond)\n",
    "conds = conds_\n",
    "print('available conditions:', conds)\n",
    "assert len(conds), 'no conditions to process'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bce15a1-3eca-46d5-a482-c82470823ce1",
   "metadata": {},
   "source": [
    "# Tally units and levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82087092-03fc-4da0-90a8-399b99911e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_dsets = {\n",
    "    cond: xr.load_dataset(rsc_path, group=input_analysis_name+'/'+cond, engine='h5netcdf')\n",
    "    for cond in conds}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "475fe161-f381-4aac-9a16-8a53ff7183e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = next(iter(cond_dsets.values()))\n",
    "names = ds['unit'].values\n",
    "ts = ds['time'].values\n",
    "for ds in cond_dsets.values():\n",
    "    assert np.array_equal(names, ds['unit'].values)\n",
    "    assert np.array_equal(ts, ds['time'].values)\n",
    "\n",
    "hier_names = unpack_hier_names(names)\n",
    "\n",
    "with h5.File(rsc_path, 'r') as f:\n",
    "    dset = f[input_analysis_name+'/unit_names']\n",
    "    all_groups_name = dset.attrs['all_groups_name'].astype(str)\n",
    "    all_groups_uid = dset.attrs['all_groups_uid']\n",
    "\n",
    "m_unit = hier_names[:,0] == 'Unit'\n",
    "n_unit = m_unit.sum()\n",
    "all_groups_idx = all_groups_uid + n_unit\n",
    "all_hier_names = np.concatenate([hier_names[m_unit], unpack_hier_names(all_groups_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167a6e5-b171-4060-ae2d-2153073e8f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "adf = pd.read_csv(annot_path).set_index('Session').loc[[sess_name]].set_index('Bank')\n",
    "\n",
    "unit_df = annot_names(\n",
    "    np.concatenate([names[(hier_names[:,0] == 'Unit')], all_groups_name]),\n",
    "    adf)\n",
    "\n",
    "m = unit_df['Level'] == 'Unit'\n",
    "unit_df['Index'] = -1\n",
    "unit_df.loc[m,'Index'] = unit_df.index[m].astype(int)\n",
    "unit_df.loc[~m,'Index'] = all_groups_idx\n",
    "\n",
    "assert unit_df['Index'].min() == 0 and unit_df['Index'].max() == len(names)-1\n",
    "unit_df['Name'] = all_hier_names[:,1]\n",
    "unit_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5eda2a4-9881-4fc2-9ffd-1d0b5cbc671c",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2719a-bbeb-4359-98f7-b112b5bd1400",
   "metadata": {},
   "outputs": [],
   "source": [
    "hiers = ['Unit', 'Channel', 'Bank', 'Array']\n",
    "bycond_lat_df = {}\n",
    "\n",
    "for cond, ds in cond_dsets.items():\n",
    "    ys_all = np.concatenate([\n",
    "        ds['sample'].values[None,...],  # shape (1n_t, n_neur)\n",
    "        ds['bootstraps'].values])       # shape (n_boot, n_t, n_neur)\n",
    "    ys_all = np.ma.masked_invalid(ys_all)\n",
    "\n",
    "    n_boot = ys_all.shape[0] - 1\n",
    "    n_neur = ys_all.shape[-1]\n",
    "    pks, tps, t2hhs, cls = (np.full((n_boot+1, n_neur), np.nan, dtype=np.float32) for i in range(4))\n",
    "\n",
    "    last_t2hh_df = None\n",
    "    done_idc = set()\n",
    "    for level in hiers[::-1]:\n",
    "        unit_df_ = unit_df[unit_df['Level'] == level]\n",
    "        if not len(unit_df_): continue\n",
    "\n",
    "        ih = hiers.index(level)\n",
    "        t2hh_df = {\n",
    "            k: np.zeros(len(unit_df_)) for k in\n",
    "            [f'{n}_{j}'\n",
    "             for j in range(n_boot+1)\n",
    "             for n in ('Latency', 'Clearance')]}\n",
    "\n",
    "        for ii, (_, row) in enumerate(tqdm(unit_df_.iterrows(), total=len(unit_df_), desc=f'{cond}, {level.lower()} level')):\n",
    "            name = row['Unit']\n",
    "            i = row['Index']\n",
    "            if i in done_idc:\n",
    "                continue\n",
    "\n",
    "            for j in range(n_boot+1):\n",
    "                hcp = 0\n",
    "                if last_t2hh_df is not None:\n",
    "                    hcp_ = hier_lookup(np.array([name]), adf, last_t2hh_df, hiers=hiers[ih+1:])\n",
    "                    if len(hcp_):\n",
    "                        assert len(hcp_) < 2\n",
    "                        if hcp_[f'Clearance_{j}'] >= clearance_thres:\n",
    "                            hcp = hcp_[f'Latency_{j}']\n",
    "\n",
    "                y = ys_all[j,:,i]\n",
    "\n",
    "                # find peak height and time\n",
    "                y_ = np.array(y)  # convert possibly masked array to array\n",
    "                if np.isfinite(y_).any():\n",
    "                    l = np.nanargmax(y_)\n",
    "                    pks[j,i] = pk = y_[l]\n",
    "                    tps[j,i] = tp = ts[l]\n",
    "                    hh = pk / 2\n",
    "                else:\n",
    "                    pk = tp = hh = np.nan\n",
    "                    pks[j,i] = tps[j,i] = np.nan\n",
    "                    continue\n",
    "\n",
    "                # find crossing point and clearance\n",
    "                t2hhs[j,i], cls[j,i] = cp, cl = \\\n",
    "                    crossing_point.get_central_crossing_point_and_clearance(\n",
    "                        ts, np.full_like(y, hh), y, x_cent=hcp, direction='up')[:2]\n",
    "                t2hh_df[f'Latency_{j}'][ii] = cp\n",
    "                t2hh_df[f'Clearance_{j}'][ii] = cl\n",
    "\n",
    "            done_idc.add(i)\n",
    "\n",
    "        df_ = pd.concat([unit_df_, pd.DataFrame(data=t2hh_df)], axis=1)\n",
    "        if last_t2hh_df is None:\n",
    "            last_t2hh_df = df_\n",
    "        else:\n",
    "            last_t2hh_df = pd.concat([df_, last_t2hh_df])\n",
    "\n",
    "\n",
    "    t2hhs, cls, pks, tps = map(np.ma.masked_invalid, (t2hhs, cls, pks, tps))\n",
    "\n",
    "    df = {}\n",
    "\n",
    "    df['Latency'] = t2hhs[0].filled(np.nan)\n",
    "    df['Clearance'] = cls[0].filled(np.nan)\n",
    "\n",
    "    df['Boots. bias, train'] = (t2hhs[0] - t2hhs[1::2].mean(0)).filled(np.nan)\n",
    "    df['Boots. stdev., train'] = t2hhs[1::2].std(0).filled(np.nan)\n",
    "    df['Boots. mean clearance, train'] = cls[1::2].mean(0).filled(0)\n",
    "    df['Boots. frac., train'] = 1 - cls[1::2].mask.mean(0)\n",
    "    df['Boots. bias'] = (t2hhs[0] - t2hhs[2::2].mean(0)).filled(np.nan)\n",
    "    df['Boots. stdev.'] = t2hhs[2::2].std(0).filled(np.nan)\n",
    "    df['Boots. mean clearance'] = cls[2::2].mean(0).filled(0)\n",
    "    df['Boots. frac.'] = 1 - cls[2::2].mask.mean(0)\n",
    "\n",
    "    df['Peak SC'] = pks[0].filled(np.nan)\n",
    "    df['Peak time'] = tps[0].filled(np.nan)\n",
    "    df['Boots. mean peak SC, train'] = pks[1::2].mean(0).filled(np.nan)\n",
    "    df['Boots. stdev. peak SC, train'] = pks[1::2].std(0).filled(np.nan)\n",
    "    df['Boots. mean peak time, train'] = tps[1::2].mean(0).filled(np.nan)\n",
    "    df['Boots. stdev. peak time, train'] = tps[1::2].std(0).filled(np.nan)\n",
    "    df['Boots. mean peak SC'] = pks[2::2].mean(0).filled(np.nan)\n",
    "    df['Boots. stdev. peak SC'] = pks[2::2].std(0).filled(np.nan)\n",
    "    df['Boots. mean peak time'] = tps[2::2].mean(0).filled(np.nan)\n",
    "    df['Boots. stdev. peak time'] = tps[2::2].std(0).filled(np.nan)\n",
    "\n",
    "    df = pd.DataFrame(df)\n",
    "    df = df.iloc[unit_df['Index'].values].reset_index().drop(columns='index')\n",
    "    df[['Level','Name']] = unit_df[['Level', 'Name']].values\n",
    "    df['Session'] = sess_name\n",
    "    df['SDF suffix'] = sdf_suffix\n",
    "    df['T step'] = t_step\n",
    "    bycond_lat_df[cond] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66638a3d-779a-4c2a-b4e8-4b50d661dc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond, df in bycond_lat_df.items():\n",
    "    df.to_hdf(\n",
    "        output_path, analysis_name+f'/{cond}/latency_dataframe',\n",
    "        mode='a', format='table', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ab242a-dbca-4040-9abf-88774b41eb5f",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e50b16a-744e-4328-9cbc-5813d8baed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(f'progress_report/{analysis_name}/all_done', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553e15c0-cf86-4e73-860d-b55bafb4bd47",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark -vm --iversions -rbg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce1fd2-1333-4e25-8c27-b9d5e9f93779",
   "metadata": {},
   "source": [
    "# Basic visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30996df3-7df9-477b-8e01-ef6f6c2573f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a923ce-6585-491e-abb0-99742b0709af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond, df in bycond_lat_df.items():\n",
    "    plt.figure()\n",
    "    sns.histplot(data=df[df['Clearance']>=clearance_thres], x='Latency', hue='Level', element='poly', fill=False, stat='density', common_norm=False)\n",
    "    plt.title(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "377093e9-0f45-4ea8-a703-bdbd7f038c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond, df in bycond_lat_df.items():\n",
    "    plt.figure()\n",
    "    sns.histplot(data=df, x='Clearance', hue='Level', element='poly', fill=False, stat='density', common_norm=False);\n",
    "    plt.title(cond)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cf73b9-70f8-48be-bd56-bc621e579da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cond, df in bycond_lat_df.items():\n",
    "    plt.figure()\n",
    "    sns.histplot(data=df, x='Boots. stdev.', hue='Level', element='poly', fill=False, stat='density', common_norm=False);\n",
    "    plt.title(cond)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
