{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a51b90-ca3a-4ea9-adf8-eb0ae0f27651",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../lib')\n",
    "from local_paths import preproc_dir, analysis_dir, database_dir, cache_dir\n",
    "from storage import get_storage_functions, quantize\n",
    "from im_patches import get_patches_from_grid\n",
    "from cross_val_pred import standardize, cv_split_by_image, cv_ridge_predict_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae9e8f6a-bb30-472d-b5d8-8fe43f2d0fb1",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9d6d3-43f1-4876-9f0e-dde9ca02a188",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# session\n",
    "#============================================================================\n",
    "sess_name = 'sess_name'\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# response windows\n",
    "#============================================================================\n",
    "# response windows\n",
    "# - one (long) window, if > 0\n",
    "t_win  =   0\n",
    "# - OTHERWISE, sliding window\n",
    "t_pre  = 200\n",
    "t_post = 375  # inclusive, but window must fit fully in range\n",
    "t_step =  10\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# fixation/saccade selection\n",
    "#============================================================================\n",
    "# fixation criteria\n",
    "ifix_sel = 2  # 0: zeroth-fix only; 1: non-zeroth-fix only; otherwise: both\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# modelling\n",
    "#============================================================================\n",
    "# ridge regularization\n",
    "ridge_alpha = 100000\n",
    "\n",
    "# cross-validation\n",
    "n_splits    =      5\n",
    "group_kfold =   True\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# paths\n",
    "#============================================================================\n",
    "proc_dir = preproc_dir\n",
    "\n",
    "# if t_win > 0:\n",
    "latency_path = database_dir + 'per_unit_latency-fix_on.csv.gz'\n",
    "# else:\n",
    "sdf_dir = preproc_dir\n",
    "sdf_suffix = '-mwa_1' if t_win > 0 else '-mwa_50'  # default to no smoothing if using a response window\n",
    "\n",
    "rf_fit_path = database_dir + 'per_unit_rf.csv.gz'\n",
    "\n",
    "feat_dir = cache_dir + 'feats/vit_large_patch16_384/blocks.13.attn.qkv'\n",
    "feat_suffix = '_as_4x4_in_1.00_steps'\n",
    "\n",
    "unit_sel_path = database_dir + 'unit_sel/visually_selective.csv.gz'\n",
    "\n",
    "output_dir = analysis_dir + 'vision_model'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a94e96-ecbd-4ec5-9089-398ee9a53ba5",
   "metadata": {},
   "source": [
    "# Check prereqs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8bc1986-32d5-4f1b-8ed0-9c578b70eb19",
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = Path(proc_dir) / (sess_name + '-proc.h5')\n",
    "print('Loading shared processing from', proc_path)\n",
    "proc_path = proc_path.expanduser()\n",
    "assert proc_path.is_file()\n",
    "\n",
    "sdf_path = Path(sdf_dir) / (sess_name + f'-sdf{sdf_suffix}.h5')\n",
    "print('Loading spike density function from', sdf_path)\n",
    "sdf_path = sdf_path.expanduser()\n",
    "assert sdf_path.is_file()\n",
    "\n",
    "with h5.File(proc_path, 'r') as f:\n",
    "    im_w, im_h = im_size = f['stimulus/size_dva'][()]\n",
    "feats_path = Path(feat_dir) / f'{im_w:.1f}x{im_h:.1f}{feat_suffix}.h5'\n",
    "print('Loading cached model features from', feats_path)\n",
    "feats_path = feats_path.expanduser()\n",
    "assert feats_path.is_file()\n",
    "\n",
    "if t_win > 0:\n",
    "    print('Using per-unit latency from', latency_path)\n",
    "    latency_path = Path(latency_path).expanduser()\n",
    "    assert latency_path.is_file()\n",
    "\n",
    "rf_fit_path = Path(rf_fit_path)\n",
    "print('Loading Gaussian-fitted RF maps density function from', rf_fit_path)\n",
    "rf_fit_path = rf_fit_path.expanduser()\n",
    "assert rf_fit_path.is_file()\n",
    "\n",
    "if unit_sel_path is not None:\n",
    "    print('Loading unit selection from', unit_sel_path)\n",
    "    unit_sel_path = Path(unit_sel_path).expanduser()\n",
    "    assert unit_sel_path.is_file()\n",
    "    unit_names = pd.read_csv(unit_sel_path).set_index('Session').loc[[sess_name]]['Unit'].values\n",
    "else:\n",
    "    unit_names = None\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "assert output_dir.expanduser().is_dir()\n",
    "output_path = output_dir / (sess_name + '.h5')\n",
    "print('Saving results to', output_path)\n",
    "output_path = output_path.expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900ce7f-95f1-4be0-a658-d3f5280298c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'vision_model'\n",
    "\n",
    "if output_path.is_file():\n",
    "    with h5.File(output_path, 'r') as f:\n",
    "        try:\n",
    "            if f[f'progress_report/{analysis_name}/all_done'][()].item():\n",
    "                raise RuntimeError(f'{sess_name} has already been processed')\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6824ab8f-bb12-4157-ad7e-76cd2bc49e67",
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_df = pd.read_hdf(proc_path, 'fixation_dataframe', 'r')\n",
    "with h5.File(proc_path, 'r') as f:\n",
    "    stim_folder_ = f['stimulus/folder'][()].decode()\n",
    "\n",
    "# Gather MD5s for images in this session.\n",
    "# The image MD5 is the index to access cached features at `feats_path`.\n",
    "# (All images in this project have unique MD5s.)\n",
    "imids = fix_df.groupby(['Image subdir', 'Image filename']).first().index\n",
    "md5s = [Path(fn).stem for _, fn in imids]  # uploaded images are already named by their MD5\n",
    "md5_catalog = pd.DataFrame(index=imids, data=md5s, columns=['MD5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40663771-7f34-40c5-9100-e4077c2bcf3b",
   "metadata": {},
   "source": [
    "# Prepare parameters; save config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9b6d9d-17ce-43e9-9663-c6bd07c550f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(proc_path, 'r') as f:\n",
    "    random_seed = f['config/default_random_seed'][()]\n",
    "\n",
    "    fix_sel = f['fixation_selection/fixation_indices'][()]\n",
    "    if ifix_sel in (0, 1):\n",
    "        m = 0 == fix_df.index.get_level_values(fix_df.index.names.index('Fixation'))[fix_sel]\n",
    "        if ifix_sel == 1:\n",
    "            m = ~m\n",
    "        fix_sel = fix_sel[m]\n",
    "\n",
    "print('random_seed:', random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086cc6c5-70f4-402d-ad17-4c61908befad",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results, add_attr_to_dset, check_equals_saved, link_dsets, copy_group = \\\n",
    "    get_storage_functions(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70683bd-b3e7-408f-8344-6aa03075031e",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/'\n",
    "save_results(group+'sdf_suffix', sdf_suffix)\n",
    "save_results(group+'fixation_selection', fix_sel)\n",
    "\n",
    "group = analysis_name + '/config/'\n",
    "save_results(group+'random_seed', random_seed)\n",
    "save_results(group+'ifix_sel', ifix_sel)\n",
    "\n",
    "group = analysis_name + '/config/time_windows/'\n",
    "save_results(group+'t_win', t_win)\n",
    "save_results(group+'t_pre', t_pre)\n",
    "save_results(group+'t_post', t_post)\n",
    "save_results(group+'t_step', t_step)\n",
    "add_attr_to_dset(group, attrs=dict(unit='ms'))\n",
    "\n",
    "group = analysis_name + '/config/modelling/'\n",
    "save_results(group+'n_splits', n_splits)\n",
    "save_results(group+'ridge_alpha', ridge_alpha)\n",
    "save_results(group+'group_kfold', group_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b95eee-7c5b-4a19-8cc9-5984f284b255",
   "metadata": {},
   "source": [
    "# Define RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9088e3c8-53a5-4f57-a28a-b7874469092b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(sdf_path, 'r') as f:\n",
    "    unit_names = f['sdf'].attrs['unit_names'].astype(str)\n",
    "    if 'unit_names' in f:\n",
    "        copy_group(f, 'unit_names', analysis_name+'/unit_names')\n",
    "\n",
    "unit_names0 = unit_names.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7fc3287-033f-4bf6-b33b-2fbddd3f5b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_rf_df = pd.read_csv(rf_fit_path).set_index('Session')\n",
    "assert sess_name in unit_rf_df.index, 'No unit has good RF fits'\n",
    "\n",
    "unit_rf_df = unit_rf_df.loc[[sess_name]].set_index('Name')\n",
    "assert not unit_rf_df.index.has_duplicates\n",
    "unit_names = unit_rf_df.index.intersection(unit_names).values\n",
    "print(f'{len(unit_names)} of {len(unit_names0)} ({(len(unit_names)/len(unit_names0))*100:.1f}%) units have fitted RFs')\n",
    "\n",
    "unit_rf_df = unit_rf_df.loc[unit_names].reset_index()\n",
    "assert len(unit_rf_df) == len(unit_names)\n",
    "print('Num units using RF fit from each source:')\n",
    "print('\\t' + '\\n\\t'.join(str(unit_rf_df.groupby('Source').count()['x']).split('\\n')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd3d3749-9018-4dd8-a99b-e228b4508bf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/unit_names', unit_names.astype(bytes))\n",
    "unit_rf_df.to_hdf(output_path, analysis_name+'/rf_per_unit', mode='a', format='table', complevel=9, complib='zlib')\n",
    "unit_rf_df['Index'] = np.arange(len(unit_rf_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ae303b-de97-4a99-aab4-5aa22e8ac745",
   "metadata": {},
   "source": [
    "# Get aligned responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ff7dc2-933e-4baf-a353-1bd0b420daa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(sdf_path, 'r') as f:\n",
    "    dset = f['sdf']\n",
    "    all_unit_names = dset.attrs['unit_names'].astype(str)\n",
    "    if unit_names is None:\n",
    "        unit_names = all_unit_names\n",
    "        unit_sel = slice(None)\n",
    "        if 'unit_names' in f:\n",
    "            copy_group(f, 'unit_names', analysis_name+'/unit_names')\n",
    "    else:\n",
    "        all_unit_names = list(all_unit_names)\n",
    "        unit_sel = np.array([v in unit_names for v in all_unit_names])\n",
    "\n",
    "    sdf = dset[()][:,unit_sel]\n",
    "\n",
    "n_neur = sdf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ebd486b-f463-4f82-b359-37489ec4be76",
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_win > 0:\n",
    "    lat_df = pd.read_csv(latency_path).set_index('Session').loc[[sess_name]].set_index('Name')\n",
    "    m = pd.Series(unit_names).isin(lat_df.index)\n",
    "    assert m.all(), f'missing latency value for {(~m).sum()} of {m.size} units'\n",
    "    assert not lat_df.index.has_duplicates\n",
    "    lat_df = lat_df.loc[unit_names].reset_index()\n",
    "    assert len(lat_df) == len(unit_names)\n",
    "    lat_df['Index'] = np.arange(len(lat_df))\n",
    "    lat_df['Latency'] = np.clip(lat_df['Latency'], 40, None)\n",
    "    print('Num units using RF fit from each source:')\n",
    "    print('\\t' + '\\n\\t'.join(str(lat_df.groupby('Source').count()['Latency']).split('\\n')[:-1]))\n",
    "    lat_df.to_hdf(output_path, analysis_name+'/latency_per_unit', mode='a', format='table', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8549a2-be05-4d9b-be5a-7a044a529ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0s = fix_df.iloc[fix_sel]['Time'].values\n",
    "if t_win > 0:\n",
    "    ts = np.array([t_win/2])  # placeholder; actual window varies with latency per unit\n",
    "else:\n",
    "    ts = np.arange(-t_pre, t_post+.1, t_step)\n",
    "resps = np.empty_like(sdf, shape=(fix_sel.size, ts.size, n_neur))\n",
    "\n",
    "if t_win > 0:\n",
    "    t_win_ = np.array([0, t_win])\n",
    "    lat_groups = [(dt, df_['Index'].values) for dt, df_ in lat_df.groupby('Latency')]\n",
    "    for i, t in enumerate(t0s):\n",
    "        for dt, usel in lat_groups:\n",
    "            s = slice(*np.round(t+dt+t_win_).astype(int))\n",
    "            resps[i,0,usel] = sdf[s,usel].mean(0)\n",
    "else:\n",
    "    for i, t in enumerate(t0s):\n",
    "        ts_ = np.round(t+ts).astype(int)\n",
    "        resps[i] = sdf[ts_,:]\n",
    "\n",
    "del sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9386ace3-4f24-4269-a3df-a59e44179189",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = standardize(resps)\n",
    "Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a82c4d-9bd7-4dc8-b437-c1800a35b82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/unit_names', unit_names.astype(bytes))\n",
    "save_results(analysis_name+'/mean_responses', resps.mean(0), attrs=dict(\n",
    "    dims=np.array(['time', 'unit'], dtype=bytes),\n",
    "    n_fix=resps.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d0cfa1-39b9-4e7a-92f4-2eb7844dbc48",
   "metadata": {},
   "source": [
    "# Define splits (group k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d9bf3f-5fa5-4054-9d7a-230f8dc2bd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "imfns = fix_df.iloc[fix_sel]['Image filename']\n",
    "\n",
    "splits, train_mask = cv_split_by_image(\n",
    "    imfns, n_splits,\n",
    "    group_kfold=group_kfold, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8b8372-8dd1-4968-87f5-e4a33fd6c11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/fix_is_train', train_mask, attrs=dict(\n",
    "    dims=np.array(['split', 'fixation'], dtype=np.bytes_),\n",
    "    random_seed=random_seed,\n",
    "    group_kfold=group_kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee5b741-342c-4e43-833c-b3b9d6e0104f",
   "metadata": {},
   "source": [
    "# Load pre-computed model reprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5944bbe-7b80-4abf-927e-05d5fe904a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(feats_path, 'r') as f:\n",
    "    patch_locs_x = f['config/patch_grid/x_locs'][()]\n",
    "    patch_locs_y = f['config/patch_grid/y_locs'][()]\n",
    "    patch_step = float(f['config/patch_grid/step'][()])\n",
    "\n",
    "    bg_feats = f['feats/bg'][()]\n",
    "\n",
    "    all_md5s = f['md5'][()].astype(str)\n",
    "    all_md5s = pd.Series(index=all_md5s, data=np.arange(len(all_md5s)), name='Index in file')\n",
    "    idc = all_md5s.loc[md5_catalog['MD5'].values]\n",
    "    md5_catalog['Index'] = np.arange(len(md5_catalog))\n",
    "\n",
    "    patch_grid_feats = np.empty(shape=(idc.size,patch_locs_x.size,patch_locs_y.size)+bg_feats.shape, dtype=bg_feats.dtype)\n",
    "    for ii, i in enumerate(idc.values):\n",
    "        patch_grid_feats[ii] = f['feats/patch_grid'][i]  # shape (n_patches_x, n_patches_y,) + feats_shape\n",
    "\n",
    "    copy_group(f, 'config', analysis_name+'/config/feats')\n",
    "\n",
    "feats_shape = bg_feats.shape\n",
    "print('Features shape:', feats_shape)\n",
    "print('Patch-grid features shape:', patch_grid_feats.shape)\n",
    "\n",
    "iims = np.array([\n",
    "    md5_catalog.loc[(row['Image subdir'], row['Image filename']), 'Index']\n",
    "    for i, (_, row) in enumerate(fix_df.iloc[fix_sel].iterrows())])\n",
    "patch_bins_x = np.concatenate([\n",
    "    patch_locs_x-patch_step/2, [patch_locs_x[-1]+patch_step/2]])\n",
    "patch_bins_y = np.concatenate([\n",
    "    patch_locs_y-patch_step/2, [patch_locs_y[-1]+patch_step/2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e407ac-337a-4f52-9cbc-b881564d7c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_stim(*args, **kwargs):\n",
    "    return get_patches_from_grid(\n",
    "        *args, patch_bins_x=patch_bins_x, patch_bins_y=patch_bins_y, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b188432-563a-4a53-a09d-16a6916d33db",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f8bbe2-4a1b-4ef8-8440-c10eaa1b79f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_win = len(ts)\n",
    "n_split = n_splits\n",
    "n_unit = len(unit_names)\n",
    "\n",
    "cv_corrs = np.full((n_win, n_unit), np.nan, dtype=np.float32)\n",
    "cv_corrs_per_split = np.full((n_win, n_split, n_unit), np.nan, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60226376-199c-4636-8047-9e530f461087",
   "metadata": {},
   "outputs": [],
   "source": [
    "xys_fix = fix_df.iloc[fix_sel][['Relative X', 'Relative Y']].values.astype(float)\n",
    "\n",
    "for igroup, ((rf_x, rf_y), df_) in enumerate(tqdm(unit_rf_df.groupby(['x', 'y']))):\n",
    "    usel = df_['Index'].values\n",
    "    X = standardize(recon_stim(iims, xys_fix+[rf_x,rf_y], patch_grid_feats, bg_feats))\n",
    "    corr, _, corr_pers, _ = cv_ridge_predict_eval(X, Y[...,usel], splits, ridge_alpha)\n",
    "    cv_corrs[:,usel] = corr\n",
    "    cv_corrs_per_split[:,:,usel] = corr_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dd3ba9-6bf8-493c-8f38-00c163cc6c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = np.array(('time', 'split', 'unit'))\n",
    "coords = dict((\n",
    "    ('time', ts),\n",
    "    ('unit', unit_names)))\n",
    "attrs = dict(\n",
    "    ifix_sel=ifix_sel,\n",
    "    n_fix=fix_sel.size,\n",
    "    group_kfold=int(group_kfold),\n",
    "    feat_shape=feats_shape,\n",
    "    ridge_alpha=ridge_alpha,\n",
    "    t_aln='fix_on')\n",
    "\n",
    "q = lambda x: quantize(x, 3)\n",
    "data_vars = {\n",
    "    'corr': (dims[[0,2]], q(cv_corrs)),\n",
    "    'corr_per_split': (dims, q(cv_corrs_per_split))}\n",
    "dataset = xr.Dataset(data_vars, coords=coords, attrs=attrs)\n",
    "\n",
    "compr = dict(zlib=True, complevel=9)\n",
    "encoding = {\n",
    "    k: dict(chunksizes=v.shape, **compr)\n",
    "    for k, v in dataset.data_vars.items()}\n",
    "dataset.to_netcdf(\n",
    "    output_path, group=analysis_name+'/data',\n",
    "    mode='a', engine='h5netcdf', encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a602f8e8-f9b3-476a-9ddd-6faaa263b251",
   "metadata": {},
   "source": [
    "# Finally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db83cdda-7a8d-4797-9f21-d649b264ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(f'progress_report/{analysis_name}/all_done', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31e09d-e580-4234-a95b-cdb1ebf28e1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark -vm --iversions -rbg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0cf607d-c810-43b0-982b-f34fd546abdf",
   "metadata": {},
   "source": [
    "# Basic visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a97760f4-1bb8-427a-91d8-049a8fb1c4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57331d16-45f1-463b-986f-e0f8cb9e017a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts, resps.mean((0,2)), '.-')\n",
    "plt.xlabel(f'Time rel. fix. on, ms')\n",
    "plt.ylabel('Grand mean FR, spks/s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62528bdc-6ae7-48d2-8ba1-6dc5fd086238",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts, np.nanmean(cv_corrs, axis=-1), '.-', c='k', lw=2, markersize=10)\n",
    "plt.plot(ts, np.nanmean(cv_corrs_per_split, axis=-1), '.-', c='gray', lw=0.5, markersize=5)\n",
    "plt.xlabel(f'Time rel. fix. on, ms')\n",
    "plt.ylabel('Model fit, Pearson\\'s r');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
