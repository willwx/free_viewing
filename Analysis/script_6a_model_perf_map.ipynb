{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../lib')\n",
    "from local_paths import preproc_dir, analysis_dir, database_dir, cache_dir\n",
    "from storage import get_storage_functions, quantize\n",
    "from im_patches import get_patches_from_grid\n",
    "from cross_val_pred import standardize, cv_split_by_image, cv_ridge_predict_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# session\n",
    "#============================================================================\n",
    "sess_name = 'sess_name'\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# spatiotemporal resolution of RF\n",
    "#============================================================================\n",
    "# temporal\n",
    "t_aln  = 'sacc_on'  # 'sacc_on' or 'fix_on'\n",
    "# response windows\n",
    "# - one (long) window, if > 0\n",
    "t_win  =   0\n",
    "# - OTHERWISE, sliding window\n",
    "t_pre  = 375\n",
    "t_post = 375  # inclusive, but window must fit fully in range\n",
    "t_step =  25\n",
    "\n",
    "# spatial\n",
    "xy_min  = -7  # dva\n",
    "xy_max  =  7\n",
    "xy_step =  1  # only meaningful if at least patch step size in cached reprs;\n",
    "              # note that patch size is set in cached reprs\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# fixation/saccade selection\n",
    "#============================================================================\n",
    "# fixation criteria\n",
    "ifix_sel        =    1    # 0: zeroth-fix only; 1: non-zeroth-fix only; otherwise: both\n",
    "select_saccades = True    # must be False if ifix_sel is None or 0\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# modelling\n",
    "#============================================================================\n",
    "# ridge regularization\n",
    "ridge_alpha = 100000\n",
    "\n",
    "# cross-validation\n",
    "n_splits    =      5\n",
    "group_kfold =   True\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# paths\n",
    "#============================================================================\n",
    "proc_dir = preproc_dir\n",
    "\n",
    "sdf_dir = preproc_dir\n",
    "sdf_suffix = '-mwa_1' if t_win > 0 else '-mwa_50'  # default to no smoothing if using a response window\n",
    "\n",
    "feat_dir = cache_dir + 'feats/vit_large_patch16_384/blocks.13.attn.qkv'\n",
    "feat_suffix = '_as_2x2_in_0.50_steps'\n",
    "\n",
    "unit_sel_path = None\n",
    "latency_path = database_dir + 'per_unit_latency-fix_on.csv.gz'  # only used # if t_win > 0\n",
    "\n",
    "output_dir = analysis_dir + 'model_perf_map'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check prereqs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = Path(proc_dir) / (sess_name + '-proc.h5')\n",
    "print('Loading shared processing from', proc_path)\n",
    "proc_path = proc_path.expanduser()\n",
    "assert proc_path.is_file()\n",
    "\n",
    "sdf_path = Path(sdf_dir) / (sess_name + f'-sdf{sdf_suffix}.h5')\n",
    "print('Loading spike density function from', sdf_path)\n",
    "sdf_path = sdf_path.expanduser()\n",
    "assert sdf_path.is_file()\n",
    "\n",
    "with h5.File(proc_path, 'r') as f:\n",
    "    im_w, im_h = im_size = f['stimulus/size_dva'][()]\n",
    "feats_path = Path(feat_dir) / f'{im_w:.1f}x{im_h:.1f}{feat_suffix}.h5'\n",
    "print('Loading cached model features from', feats_path)\n",
    "feats_path = feats_path.expanduser()\n",
    "assert feats_path.is_file()\n",
    "\n",
    "if t_win > 0:\n",
    "    print('Using per-unit latency from', latency_path)\n",
    "    latency_path = Path(latency_path).expanduser()\n",
    "    assert latency_path.is_file()\n",
    "\n",
    "if unit_sel_path is not None:\n",
    "    print('Loading unit selection from', unit_sel_path)\n",
    "    unit_sel_path = Path(unit_sel_path).expanduser()\n",
    "    assert unit_sel_path.is_file()\n",
    "    unit_names = pd.read_csv(unit_sel_path).set_index('Session').loc[[sess_name]]['Unit'].values\n",
    "else:\n",
    "    unit_names = None\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "assert output_dir.expanduser().is_dir()\n",
    "output_path = output_dir / (sess_name + '.h5')\n",
    "print('Saving results to', output_path)\n",
    "output_path = output_path.expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'model_perf_map'\n",
    "\n",
    "if output_path.is_file():\n",
    "    with h5.File(output_path, 'r') as f:\n",
    "        try:\n",
    "            if f[f'progress_report/{analysis_name}/all_done'][()].item():\n",
    "                raise RuntimeError(f'{sess_name} has already been processed')\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_df = pd.read_hdf(proc_path, 'fixation_dataframe', 'r')\n",
    "with h5.File(proc_path, 'r') as f:\n",
    "    stim_folder_ = f['stimulus/folder'][()].decode()\n",
    "\n",
    "imids = fix_df.groupby(['Image subdir', 'Image filename']).first().index\n",
    "md5s = [Path(fn).stem for _, fn in imids]\n",
    "md5_catalog = pd.DataFrame(index=imids, data=md5s, columns=['MD5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare parameters; save config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(proc_path, 'r') as f:\n",
    "    random_seed = f['config/default_random_seed'][()]\n",
    "print('random_seed:', random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(proc_path, 'r') as f:\n",
    "    if select_saccades:\n",
    "        assert ifix_sel == 1, 'if selecting saccades, must only include non-zeroth fixations'\n",
    "        fix1_sel, fix2_sel = f['saccade_selection/fixation_indices'][()]\n",
    "    else:\n",
    "        assert t_aln == 'fix_on'\n",
    "        fix2_sel = f['fixation_selection/fixation_indices'][()]\n",
    "        if ifix_sel in (0, 1):\n",
    "            m = 0 == fix_df.index.get_level_values(fix_df.index.names.index('Fixation'))[fix2_sel]\n",
    "            if ifix_sel == 1:\n",
    "                m = ~m\n",
    "            fix2_sel = fix2_sel[m]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert t_aln in ('sacc_on', 'fix_on')\n",
    "if t_win > 0:\n",
    "    ts = np.array([t_win/2])  # placeholder; actual window varies with latency per unit\n",
    "else:\n",
    "    ts = np.arange(-t_pre, t_post+.1, t_step)\n",
    "print(f'Response windows ({len(ts)}): {ts}')\n",
    "\n",
    "rf_locs = np.arange(xy_min, xy_max+xy_step/10, xy_step)\n",
    "print(f'RF locations ({len(rf_locs)}): {rf_locs}')\n",
    "tb = (rf_locs[0]-xy_step/2, rf_locs[-1]+xy_step/2)\n",
    "rf_extent = tb + tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results, add_attr_to_dset, check_equals_saved, link_dsets, copy_group = \\\n",
    "    get_storage_functions(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/'\n",
    "save_results(group+'ts', ts, attrs=dict(unit='ms'))\n",
    "save_results(group+'rf_locs', rf_locs, attrs=dict(unit='dva'))\n",
    "save_results(group+'sdf_suffix', sdf_suffix)\n",
    "\n",
    "group = analysis_name + '/config/'\n",
    "save_results(group+'random_seed', random_seed)\n",
    "\n",
    "group = analysis_name + '/config/time_windows/'\n",
    "save_results(group+'t_aln', t_aln)\n",
    "save_results(group+'t_win', t_win)\n",
    "save_results(group+'t_pre', t_pre)\n",
    "save_results(group+'t_post', t_post)\n",
    "save_results(group+'t_step', t_step)\n",
    "add_attr_to_dset(group, attrs=dict(unit='ms'))\n",
    "\n",
    "group = analysis_name + '/config/retinotopic_locations/'\n",
    "save_results(group+'xy_min', xy_min)\n",
    "save_results(group+'xy_max', xy_max)\n",
    "save_results(group+'xy_step', xy_step)\n",
    "add_attr_to_dset(group, attrs=dict(unit='dva'))\n",
    "\n",
    "group = analysis_name + '/config/fixation_selection/'\n",
    "save_results(group+'ifix_sel', ifix_sel)\n",
    "save_results(group+'select_saccades', select_saccades)\n",
    "\n",
    "group = analysis_name + '/config/modelling/'\n",
    "save_results(group+'n_splits', n_splits)\n",
    "save_results(group+'ridge_alpha', ridge_alpha)\n",
    "save_results(group+'group_kfold', group_kfold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For saccades, construct two types of control points\n",
    "- Control 1: the 3rd vertex of two equilateral triangles anchored by fix 1 & 2 (previous and current), i.e., the L4/5 Langange points. One of the points is chosen:\n",
    "  - If both points are within image bounds, choose randomly\n",
    "  - If one and only one point is within image bounds, choose that\n",
    "  - If both points are outside image bounds, choose the one closest to the image.\n",
    "- Control 2: the midpoint between the previous and current fixation, i.e., the L1 point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xys_fix2 = fix_df.iloc[fix2_sel][['Relative X', 'Relative Y']].values.astype(float)\n",
    "if select_saccades:\n",
    "    xys_fix1 = fix_df.iloc[fix1_sel][['Relative X', 'Relative Y']].values.astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not select_saccades:\n",
    "    n_cond = 1\n",
    "\n",
    "else:\n",
    "    n_cond = 4\n",
    "    rg = np.random.default_rng(random_seed)\n",
    "\n",
    "    # control point type 2: midpoint between fixs 1, 2\n",
    "    xys_control2 = (xys_fix1 + xys_fix2) / 2\n",
    "\n",
    "    # control point type 1\n",
    "    # - get 3rd vertex of the equilateral triangle\n",
    "    #   by rotating the saccade vector +/-60 degs\n",
    "    vs = xys_fix2 - xys_fix1\n",
    "    rmats = [\n",
    "        np.array([[np.cos(a), -np.sin(a)], [np.sin(a), np.cos(a)]])\n",
    "        for a in (np.pi/3, -np.pi/3)\n",
    "    ]\n",
    "    xys_l45 = xys_fix1 + np.array([(m @ vs.T).T for m in rmats])  # shape (n_fix, 2)\n",
    "\n",
    "    # - check whether/how far points are out of image bounds\n",
    "    d2b = np.array([\n",
    "        [\n",
    "            np.max([xys[:,j]-im_size[j]/2, -im_size[j]/2-xys[:,j]], axis=0)\n",
    "            for j in range(2)\n",
    "        ]\n",
    "        for xys in xys_l45\n",
    "    ])  # shape (2, 2, n_fix)\n",
    "    oob = np.any(d2b >= 0, axis=1)  # shape (2, n_fix)\n",
    "\n",
    "    # - choose one of the two candidate control points according to 1 of 3 conditions\n",
    "    m0 = np.all(oob, axis=0)\n",
    "    m2 = np.all(~oob, axis=0)\n",
    "    m1 = ~(m0 | m2)\n",
    "    xys_control1 = np.empty_like(xys_fix1)\n",
    "    if np.any(m0):\n",
    "        xys_control1[m0] = [xys_l45[np.argmin(d2b[...,i].max(1)), i] for i in np.nonzero(m0)[0]]\n",
    "    if np.any(m1):\n",
    "        xys_control1[m1] = [xys_l45[np.argmin(oob[:,i]), i] for i in np.nonzero(m1)[0]]\n",
    "    if np.any(m2):\n",
    "        rand_par = (rg.random(size=m2.sum()) < 0.5).astype(int)\n",
    "        xys_control1[m2] = [xys_l45[p, i] for p, i in zip(rand_par, np.nonzero(m2)[0])]\n",
    "\n",
    "    # - quality control\n",
    "    for i, xys in enumerate((xys_fix1, xys_fix2, xys_control1, xys_control2)):\n",
    "        if i < 2:\n",
    "            # sanity check: control point must be equidistant to both fixations\n",
    "            assert np.allclose(np.linalg.norm(xys_control1 - xys, axis=1), np.linalg.norm(vs, axis=1))\n",
    "\n",
    "        d2b_ = np.array([\n",
    "            np.max([xys[:,j]-im_size[j]/2, -im_size[j]/2-xys[:,j]], axis=0)\n",
    "            for j in range(2)\n",
    "        ])\n",
    "        m = np.any(d2b_ >= 0, 0)\n",
    "        d2b__ = d2b_[:,m].max(0)\n",
    "        cond = ('Fixation 1', 'Fixation 2', 'Control 1 (equidist)', 'Control 2 (midpoint)')[i]\n",
    "        print(f'{cond +\":\":<18}\\t{m.mean()*100:4.1f}% out of image boundary', end='')\n",
    "        if np.any(m):\n",
    "            print(f'; d = {d2b__.mean():.1f} +/- {d2b__.std():.1f}')\n",
    "        else:\n",
    "            print()\n",
    "        d2b__ = -(d2b_.max(0))\n",
    "        print(f'\\tmean min dist to bound: {d2b__.mean():.1f} +/- {d2b__.std():.1f} dva')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_frames = np.array(('fix1', 'fix2', 'equidistant', 'midpoint')[:n_cond])\n",
    "if select_saccades:\n",
    "    xy_degs_conds = (xys_fix1, xys_fix2, xys_control1, xys_control2)\n",
    "else:\n",
    "    xy_degs_conds = (xys_fix2,)\n",
    "    ref_frames = ref_frames[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/fix_sel'\n",
    "if select_saccades:\n",
    "    save_results(\n",
    "        group, np.array([fix1_sel, fix2_sel]),\n",
    "        attrs=dict(dims=np.array(['fix_1_or_2', 'index'], dtype=np.bytes_)))\n",
    "else:\n",
    "    save_results(\n",
    "        group, fix2_sel,\n",
    "        attrs=dict(dims=np.array(['index'], dtype=np.bytes_)))\n",
    "\n",
    "if select_saccades:\n",
    "    group = analysis_name + '/control_points/'\n",
    "    attrs = dict(dims=np.array(['index', 'xy'], dtype=np.bytes_), unit='dva', comment='Relative to image center')\n",
    "    save_results(group+'equidistant', xys_control1, attrs=attrs)\n",
    "    save_results(group+'midpoint', xys_control2, attrs=attrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get aligned responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(sdf_path, 'r') as f:\n",
    "    dset = f['sdf']\n",
    "    all_unit_names = dset.attrs['unit_names'].astype(str)\n",
    "    if unit_names is None:\n",
    "        unit_names = all_unit_names\n",
    "        unit_sel = slice(None)\n",
    "        if 'unit_names' in f:\n",
    "            copy_group(f, 'unit_names', analysis_name+'/unit_names')\n",
    "    else:\n",
    "        all_unit_names = list(all_unit_names)\n",
    "        unit_sel = np.array([v in unit_names for v in all_unit_names])\n",
    "\n",
    "    sdf = dset[()][:,unit_sel]\n",
    "\n",
    "n_neur = sdf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_win > 0:\n",
    "    lat_df = pd.read_csv(latency_path).set_index('Session').loc[[sess_name]].set_index('Name')\n",
    "    m = pd.Series(unit_names).isin(lat_df.index)\n",
    "    assert m.all(), f'missing latency values for {(~m).sum()} of {m.size} units'\n",
    "    assert not lat_df.index.has_duplicates\n",
    "    lat_df = lat_df.loc[unit_names].reset_index()\n",
    "    assert len(lat_df) == len(unit_names)\n",
    "    lat_df['Index'] = np.arange(len(lat_df))\n",
    "    lat_df['Latency'] = np.clip(lat_df['Latency'], 40, None)\n",
    "    print('Num units using RF fit from each source:')\n",
    "    print('\\t' + '\\n\\t'.join(str(lat_df.groupby('Source').count()['Latency']).split('\\n')[:-1]))\n",
    "    lat_df.to_hdf(output_path, analysis_name+'/latency_per_unit', mode='a', format='table', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_aln == 'sacc_on':\n",
    "    t0s = fix_df.iloc[fix1_sel][['Time', 'Duration']].values.sum(1)\n",
    "else:\n",
    "    t0s = fix_df.iloc[fix2_sel]['Time'].values\n",
    "\n",
    "resps = np.empty_like(sdf, shape=(fix2_sel.size, ts.size, n_neur))\n",
    "\n",
    "if t_win > 0:\n",
    "    t_win_ = np.array([0, t_win])\n",
    "    lat_groups = [(dt, df_['Index'].values) for dt, df_ in lat_df.groupby('Latency')]\n",
    "    for i, t in enumerate(t0s):\n",
    "        for dt, usel in lat_groups:\n",
    "            s = slice(*np.round(t+dt+t_win_).astype(int))\n",
    "            resps[i,0,usel] = sdf[s,usel].mean(0)\n",
    "else:\n",
    "    for i, t in enumerate(t0s):\n",
    "        ts_ = np.round(t+ts).astype(int)\n",
    "        resps[i] = sdf[ts_,:]\n",
    "\n",
    "del sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = standardize(resps)\n",
    "Y.shape, Y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/unit_names', unit_names.astype(bytes))\n",
    "save_results(analysis_name+'/mean_responses', resps.mean(0), attrs=dict(\n",
    "    dims=np.array(['time', 'unit'], dtype=bytes),\n",
    "    n_fix=resps.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define splits (group k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imfns = fix_df.iloc[fix2_sel]['Image filename']\n",
    "\n",
    "splits, train_mask = cv_split_by_image(\n",
    "    imfns, n_splits,\n",
    "    group_kfold=group_kfold, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/fix_is_train', train_mask, attrs=dict(\n",
    "    dims=np.array(['split', 'fixation'], dtype=np.bytes_),\n",
    "    random_seed=random_seed,\n",
    "    group_kfold=group_kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-computed model reprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(feats_path, 'r') as f:\n",
    "    patch_locs_x = f['config/patch_grid/x_locs'][()]\n",
    "    patch_locs_y = f['config/patch_grid/y_locs'][()]\n",
    "    patch_step = float(f['config/patch_grid/step'][()])\n",
    "\n",
    "    bg_feats = f['feats/bg'][()]\n",
    "\n",
    "    all_md5s = f['md5'][()].astype(str)\n",
    "    all_md5s = pd.Series(index=all_md5s, data=np.arange(len(all_md5s)), name='Index in file')\n",
    "    idc = all_md5s.loc[md5_catalog['MD5'].values]\n",
    "    md5_catalog['Index'] = np.arange(len(md5_catalog))\n",
    "\n",
    "    patch_grid_feats = np.empty(shape=(idc.size,patch_locs_x.size,patch_locs_y.size)+bg_feats.shape, dtype=bg_feats.dtype)\n",
    "    for ii, i in enumerate(idc.values):\n",
    "        patch_grid_feats[ii] = f['feats/patch_grid'][i]  # shape (n_patches_x, n_patches_y,) + feats_shape\n",
    "\n",
    "    copy_group(f, 'config', analysis_name+'/config/feats')\n",
    "\n",
    "feats_shape = bg_feats.shape\n",
    "print('Features shape:', feats_shape)\n",
    "print('Patch-grid features shape:', patch_grid_feats.shape)\n",
    "\n",
    "iims = np.array([\n",
    "    md5_catalog.loc[(row['Image subdir'], row['Image filename']), 'Index']\n",
    "    for i, (_, row) in enumerate(fix_df.iloc[fix2_sel].iterrows())])\n",
    "patch_bins_x = np.concatenate([\n",
    "    patch_locs_x-patch_step/2, [patch_locs_x[-1]+patch_step/2]])\n",
    "patch_bins_y = np.concatenate([\n",
    "    patch_locs_y-patch_step/2, [patch_locs_y[-1]+patch_step/2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_stim(*args, **kwargs):\n",
    "    return get_patches_from_grid(\n",
    "        *args, patch_bins_x=patch_bins_x, patch_bins_y=patch_bins_y, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_win = len(ts)\n",
    "n_split = n_splits\n",
    "n_loc = len(rf_locs)\n",
    "n_unit = len(unit_names)\n",
    "\n",
    "cv_corrs = np.full((n_cond, n_win, n_loc, n_loc, n_unit), np.nan, dtype=np.float32)\n",
    "cv_r2s = np.full_like(cv_corrs, np.nan)\n",
    "cv_corrs_per_split = np.full((n_cond, n_win, n_split, n_loc, n_loc, n_unit), np.nan, dtype=np.float32)\n",
    "cv_r2s_per_split = np.full_like(cv_corrs_per_split, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (ic, xy_degs), (iy, dy), (ix, dx) in tqdm(\n",
    "        itertools.product(enumerate(xy_degs_conds), enumerate(rf_locs), enumerate(rf_locs)),\n",
    "        total=n_cond*n_loc**2):\n",
    "\n",
    "    X = standardize(recon_stim(iims, xy_degs+[dx,dy], patch_grid_feats, bg_feats))\n",
    "\n",
    "    corr, r2, corr_pers, r2_pers = cv_ridge_predict_eval(X, Y, splits, ridge_alpha)\n",
    "\n",
    "    cv_corrs[ic,:,iy,ix,:] = corr\n",
    "    cv_r2s[ic,:,iy,ix,:] = r2\n",
    "    cv_corrs_per_split[ic,:,:,iy,ix,:] = corr_pers\n",
    "    cv_r2s_per_split[ic,:,:,iy,ix,:] = r2_pers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = np.array(('ref_frame', 'time', 'split', 'rf_y', 'rf_x', 'unit'))\n",
    "coords = dict((\n",
    "    ('ref_frame', ref_frames),\n",
    "    ('time', ts),\n",
    "    ('rf_y', rf_locs),\n",
    "    ('rf_x', rf_locs),\n",
    "    ('unit', unit_names)))\n",
    "attrs = dict(\n",
    "    ifix_sel=ifix_sel,\n",
    "    n_fix=fix2_sel.size,\n",
    "    group_kfold=int(group_kfold),\n",
    "    feat_shape=feats_shape,\n",
    "    ridge_alpha=ridge_alpha,\n",
    "    t_aln=t_aln)\n",
    "\n",
    "q = lambda x: quantize(x, 3)\n",
    "data_vars = {\n",
    "    'corr': (dims[[0,1,3,4,5]], q(cv_corrs)),\n",
    "    'r2': (dims[[0,1,3,4,5]], q(cv_r2s)),\n",
    "    'corr_per_split': (dims, q(cv_corrs_per_split)),\n",
    "    'r2_per_split': (dims, q(cv_r2s_per_split))}\n",
    "dataset = xr.Dataset(data_vars, coords=coords, attrs=attrs)\n",
    "\n",
    "compr = dict(zlib=True, complevel=9)\n",
    "encoding = {\n",
    "    k: dict(chunksizes=v.shape, **compr)\n",
    "    for k, v in dataset.data_vars.items()}\n",
    "dataset.to_netcdf(\n",
    "    output_path, group=analysis_name+'/data',\n",
    "    mode='a', engine='h5netcdf', encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(f'progress_report/{analysis_name}/all_done', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark -vm --iversions -rbg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = {'Condition': [], 'X': [], 'Y': []}\n",
    "for i, xys in enumerate(xy_degs_conds):\n",
    "    if select_saccades:\n",
    "        cond = ('Fixation 1', 'Fixation 2', 'Equidistant control', 'Midpoint control')[i]\n",
    "    else:\n",
    "        cond = 'Default'\n",
    "    df['Condition'].append(np.full(len(xys), cond))\n",
    "    df['X'].append(xys[:,0])\n",
    "    df['Y'].append(xys[:,1])\n",
    "for k in df.keys():\n",
    "    df[k] = np.concatenate(df[k])\n",
    "df = pd.DataFrame(df)\n",
    "ax = sns.kdeplot(data=df, x='X', y='Y', hue='Condition', levels=3)\n",
    "ax.set_title('Distribution of selected fixations')\n",
    "ax.set_xlabel('X, dva')\n",
    "ax.set_ylabel('Y, dva')\n",
    "ax.set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if select_saccades:\n",
    "    sacc_vecs = (\n",
    "        fix_df.iloc[fix2_sel][['Relative X', 'Relative Y']].values\n",
    "        - fix_df.iloc[fix1_sel][['Relative X', 'Relative Y']].values)\n",
    "    plt.hist2d(x=sacc_vecs[:,0], y=sacc_vecs[:,1], cmap='gist_gray_r')\n",
    "    plt.xlabel('X, dva')\n",
    "    plt.ylabel('Y, dva')\n",
    "    plt.title('Distribution of selected saccade vectors')\n",
    "    plt.gca().set_aspect('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(ts, resps.mean((0,2)), '.-')\n",
    "plt.xlabel(f'Time aligned to {t_aln}, ms')\n",
    "plt.ylabel('Grand mean FR, spks/s');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = abs(np.nanpercentile(np.nanmean(cv_corrs,-1), 99.5))\n",
    "\n",
    "i = np.argmin(np.abs(ts - 200))\n",
    "map_ = np.ma.masked_invalid(cv_corrs[0,i,:,:,:]).mean(-1).filled(np.nan)\n",
    "cm = plt.imshow(map_, cmap='RdGy', origin='lower', extent=rf_extent, vmin=-vr, vmax=vr)\n",
    "cb = plt.colorbar(cm)\n",
    "cb.ax.set_ylabel('Model fit, Pearson\\'s r')\n",
    "plt.xlabel('X, dva')\n",
    "plt.ylabel('Y, dva')\n",
    "if select_saccades:\n",
    "    plt.title(f'Fix 1-aligned model performance, {ts[i]} ms')\n",
    "else:\n",
    "    plt.title(f'Fixation-aligned model performance, {ts[i]} ms')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if select_saccades:\n",
    "    i = np.argmin(np.abs(ts - 200))\n",
    "    map_ = np.ma.masked_invalid(cv_corrs[1,i,:,:,:]).mean(-1).filled(np.nan)\n",
    "    cm = plt.imshow(map_, cmap='RdGy', origin='lower', extent=rf_extent, vmin=-vr, vmax=vr)\n",
    "    cb = plt.colorbar(cm)\n",
    "    cb.ax.set_ylabel('Model fit, Pearson\\'s r')\n",
    "    plt.xlabel('X, dva')\n",
    "    plt.ylabel('Y, dva')\n",
    "    plt.title(f'Fix 2-aligned model performance, {ts[i]} ms');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if select_saccades:\n",
    "    i = np.argmin(np.abs(ts))\n",
    "    map_ = np.ma.masked_invalid(cv_corrs[1,i,:,:,:]).mean(-1).filled(np.nan)\n",
    "    cm = plt.imshow(map_, cmap='RdGy', origin='lower', extent=rf_extent, vmin=-vr, vmax=vr)\n",
    "    cb = plt.colorbar(cm)\n",
    "    cb.ax.set_ylabel('Model fit, Pearson\\'s r')\n",
    "    plt.xlabel('X, dva')\n",
    "    plt.ylabel('Y, dva')\n",
    "    plt.title(f'Fix 2-aligned model performance, {ts[i]} ms');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 3 * cv_corrs.shape[0]\n",
    "w = min(h / cv_corrs.shape[0] * cv_corrs.shape[1], 20)\n",
    "h = min(h, w / cv_corrs.shape[1] * cv_corrs.shape[0])\n",
    "plt.figure(figsize=(w,h))\n",
    "a = np.ma.masked_invalid(cv_corrs).mean(-1)\n",
    "a = a.filled(np.nan)\n",
    "a = a[:,:,::-1,:]  # flip y such that the first rows now correspond to larger y\n",
    "a = np.hstack(np.hstack(a))\n",
    "plt.imshow(a, cmap='RdGy', vmin=-vr, vmax=vr);\n",
    "ax = plt.gca()\n",
    "ax.set_xticks([])\n",
    "ax.set_yticks([])\n",
    "[v.set_visible(False) for v in ax.spines.values()]\n",
    "plt.xlabel('Response window')\n",
    "plt.ylabel('Reference frame')\n",
    "plt.title('Mean 3D map per condition');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
