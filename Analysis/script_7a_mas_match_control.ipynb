{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import itertools\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py as h5\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from tqdm import tqdm\n",
    "\n",
    "sys.path.append('../lib')\n",
    "from local_paths import preproc_dir, analysis_dir, database_dir, cache_dir\n",
    "from storage import get_storage_functions, quantize\n",
    "from hier_group import unpack_hier_names\n",
    "from im_patches import get_patches_from_grid\n",
    "from cross_val_pred import standardize, cv_split_by_image, cv_ridge_predict_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "#============================================================================\n",
    "# session\n",
    "#============================================================================\n",
    "sess_name = 'sess_name'\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# saccade-matching\n",
    "#============================================================================\n",
    "min_sep = 4  # for non-matched fixation\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# spatiotemporal resolution of RF\n",
    "#============================================================================\n",
    "# temporal\n",
    "t_aln  = 'sacc_on'  # 'sacc_on' or 'fix_on'\n",
    "t_win  = 0  # if > 0, uses single window\n",
    "t_offset = None  # default: -t_win/2; time bin center = t_offset + t_loc or timing\n",
    "# - if t_win > 0\n",
    "t_loc  = None  # if set, supercedes timing_path and t_col\n",
    "# -- if t_loc is None\n",
    "t_col  = None\n",
    "# - if t_win <= 0\n",
    "t_pre  = 375\n",
    "t_post = 375  # inclusive, but window must fit fully in range\n",
    "t_step =  25\n",
    "\n",
    "# spatial\n",
    "rloc_min  = -0.5  # normalized to saccade length\n",
    "rloc_max  =  1.5\n",
    "rloc_step =  0.25\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# modelling\n",
    "#============================================================================\n",
    "# ridge regularization\n",
    "ridge_alpha = 100000\n",
    "\n",
    "# cross-validation\n",
    "n_splits    =      5\n",
    "group_kfold =   True\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# statistical tests\n",
    "#============================================================================\n",
    "n_perm = 0\n",
    "match_test = ('original', 'previous', 0, 'greater')\n",
    "\n",
    "\n",
    "#============================================================================\n",
    "# paths\n",
    "#============================================================================\n",
    "proc_dir = preproc_dir\n",
    "\n",
    "sdf_dir = preproc_dir\n",
    "sdf_suffix = '-mwa_1' if t_win > 0 else '-mwa_50'  # default to no smoothing if using a response window\n",
    "\n",
    "rsc_dir = analysis_dir + 'self_consistency_no_decorr'\n",
    "\n",
    "feat_dir = cache_dir + 'feats/vit_large_patch16_384/blocks.13.attn.qkv'\n",
    "feat_suffix = '_as_2x2_in_0.50_steps'\n",
    "\n",
    "unit_sel_path = database_dir+'unit_sel/visually_selective.csv.gz'\n",
    "rf_fit_path = database_dir + 'per_unit_rf.csv.gz'\n",
    "timing_path = database_dir + 'timing-mpm_peaks.csv.gz'\n",
    "\n",
    "output_dir = analysis_dir + 'mas_match_control'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check prereqs and params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_path = Path(proc_dir) / (sess_name + '-proc.h5')\n",
    "print('Loading shared processing from', proc_path)\n",
    "proc_path = proc_path.expanduser()\n",
    "assert proc_path.is_file()\n",
    "\n",
    "sdf_path = Path(sdf_dir) / (sess_name + f'-sdf{sdf_suffix}.h5')\n",
    "print('Loading spike density function from', sdf_path)\n",
    "sdf_path = sdf_path.expanduser()\n",
    "assert sdf_path.is_file()\n",
    "\n",
    "rsc_path = Path(rsc_dir) / (sess_name + '.h5')\n",
    "print('Loading return fixation self-consistency results from', rsc_path)\n",
    "rsc_path = rsc_path.expanduser()\n",
    "assert rsc_path.is_file()\n",
    "\n",
    "with h5.File(proc_path, 'r') as f:\n",
    "    im_w, im_h = im_size = f['stimulus/size_dva'][()]\n",
    "feats_path = Path(feat_dir) / f'{im_w:.1f}x{im_h:.1f}{feat_suffix}.h5'\n",
    "print('Loading cached model features from', feats_path)\n",
    "feats_path = feats_path.expanduser()\n",
    "assert feats_path.is_file()\n",
    "\n",
    "rf_fit_path = Path(rf_fit_path)\n",
    "print('Loading Gaussian-fitted RF maps density function from', rf_fit_path)\n",
    "rf_fit_path = rf_fit_path.expanduser()\n",
    "assert rf_fit_path.is_file()\n",
    "\n",
    "if unit_sel_path is not None and unit_sel_path != 'None':\n",
    "    print('Loading unit selection from', unit_sel_path)\n",
    "    unit_sel_path = Path(unit_sel_path).expanduser()\n",
    "    assert unit_sel_path.is_file()\n",
    "    unit_names = pd.read_csv(unit_sel_path).set_index('Session').loc[[sess_name]]['Unit'].values\n",
    "else:\n",
    "    unit_names = None\n",
    "\n",
    "if t_win > 0:\n",
    "    t_pre = t_post = t_step = np.nan\n",
    "    if t_offset is None: t_offset = -t_win / 2\n",
    "    if t_loc is None:\n",
    "        assert timing_path is not None and t_col is not None\n",
    "        t_loc = np.nan\n",
    "        print('Using per-unit timing from', timing_path)\n",
    "        print('\\tColumn:', t_col, '\\tOffset:', t_offset)\n",
    "        timing_path = Path(timing_path).expanduser()\n",
    "        assert timing_path.is_file()\n",
    "    else:\n",
    "        t_col = ''\n",
    "else:\n",
    "    t_loc = t_offset = np.nan\n",
    "    t_col = ''\n",
    "\n",
    "output_dir = Path(output_dir)\n",
    "assert output_dir.expanduser().is_dir()\n",
    "output_path = output_dir / (sess_name + '.h5')\n",
    "print('Saving results to', output_path)\n",
    "output_path = output_path.expanduser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_perm > 0:\n",
    "    if isinstance(match_test, str):\n",
    "        match_test = ast.literal_eval(match_test)\n",
    "    else:\n",
    "        assert isinstance(match_test, tuple)\n",
    "    assert match_test[0] == 'original'  # placeholder, for consistency\n",
    "    assert match_test[1] in ('previous', 'current')\n",
    "    assert isinstance(match_test[3], str)  # alternative to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_name = 'mas_match_control'\n",
    "\n",
    "if output_path.is_file():\n",
    "    with h5.File(output_path, 'r') as f:\n",
    "        try:\n",
    "            if f[f'progress_report/{analysis_name}/all_done'][()].item():\n",
    "                raise RuntimeError(f'{sess_name} has already been processed')\n",
    "        except KeyError:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_df = pd.read_hdf(proc_path, 'fixation_dataframe', 'r')\n",
    "with h5.File(proc_path, 'r') as f:\n",
    "    stim_folder_ = f['stimulus/folder'][()].decode()\n",
    "\n",
    "imids = fix_df.groupby(['Image subdir', 'Image filename']).first().index\n",
    "md5s = [Path(fn).stem for _, fn in imids]\n",
    "md5_catalog = pd.DataFrame(index=imids, data=md5s, columns=['MD5'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare parameters; save config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(proc_path, 'r') as f:\n",
    "    random_seed = f['config/default_random_seed'][()]\n",
    "print('random_seed:', random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert t_aln in ('sacc_on', 'fix_on')\n",
    "if t_win > 0:\n",
    "    ts = np.array([t_win/2]) + t_offset  # placeholder; actual window varies with latency per unit\n",
    "else:\n",
    "    ts = np.arange(-t_pre, t_post+.1, t_step)\n",
    "print(f'{len(ts)} response windows')\n",
    "\n",
    "rf_rlocs = np.arange(rloc_min, rloc_max+rloc_step/10, rloc_step)\n",
    "if n_perm > 0 and match_test[2] is not None:\n",
    "    assert match_test[2] in rf_rlocs  # control loc in stat test is unique to this analysis\n",
    "print('RF locations (along saccade vector, normalized by saccade size):')\n",
    "print(f'{rf_rlocs} ({len(rf_rlocs)})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results, add_attr_to_dset, check_equals_saved, link_dsets, copy_group = \\\n",
    "    get_storage_functions(output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/'\n",
    "save_results(group+'ts', ts, attrs=dict(unit='ms'))\n",
    "save_results(group+'rf_rlocs', rf_rlocs, attrs=dict(unit='normalized'))\n",
    "save_results(group+'sdf_suffix', sdf_suffix)\n",
    "\n",
    "group = analysis_name + '/config/'\n",
    "save_results(group+'random_seed', random_seed)\n",
    "\n",
    "group = analysis_name + '/config/match_criterion/'\n",
    "save_results(group+'min_sep', min_sep)\n",
    "\n",
    "group = analysis_name + '/config/time_windows/'\n",
    "save_results(group+'t_aln', t_aln)\n",
    "save_results(group+'t_win', t_win)\n",
    "save_results(group+'t_loc', t_loc)\n",
    "save_results(group+'t_col', t_col)\n",
    "save_results(group+'t_offset', t_offset)\n",
    "save_results(group+'t_pre', t_pre)\n",
    "save_results(group+'t_post', t_post)\n",
    "save_results(group+'t_step', t_step)\n",
    "add_attr_to_dset(group, attrs=dict(unit='ms'))\n",
    "\n",
    "group = analysis_name + '/config/positions_along_saccade/'\n",
    "save_results(group+'rloc_min', rloc_min)\n",
    "save_results(group+'rloc_max', rloc_max)\n",
    "save_results(group+'rloc_step', rloc_step)\n",
    "add_attr_to_dset(group, attrs=dict(unit='normalized'))\n",
    "\n",
    "group = analysis_name + '/config/modelling/'\n",
    "save_results(group+'n_splits', n_splits)\n",
    "save_results(group+'ridge_alpha', ridge_alpha)\n",
    "save_results(group+'group_kfold', group_kfold)\n",
    "\n",
    "group = analysis_name + '/config/statistics/'\n",
    "save_results(group+'n_perm', n_perm)\n",
    "save_results(group+'match_test', str(match_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define RFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if unit_names is None:\n",
    "    with h5.File(sdf_path, 'r') as f:\n",
    "        unit_names = f['sdf'].attrs['unit_names'].astype(str)\n",
    "\n",
    "unit_names0 = unit_names.copy()\n",
    "unit_names = unpack_hier_names(unit_names)\n",
    "unit_names = unit_names[unit_names[:,0]=='Unit', 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_rf_df = pd.read_csv(rf_fit_path).set_index('Session')\n",
    "assert sess_name in unit_rf_df.index, 'No unit has good RF fits'\n",
    "\n",
    "unit_rf_df = unit_rf_df.loc[[sess_name]].set_index('Name')\n",
    "assert not unit_rf_df.index.has_duplicates\n",
    "unit_names = unit_rf_df.index.intersection(unit_names).values\n",
    "print(f'{len(unit_names)} of {len(unit_names0)} ({(len(unit_names)/len(unit_names0))*100:.1f}%) units have fitted RFs')\n",
    "\n",
    "unit_rf_df = unit_rf_df.loc[unit_names].reset_index()\n",
    "assert len(unit_rf_df) == len(unit_names)\n",
    "print('Num units using RF fit from each source:')\n",
    "print('\\t' + '\\n\\t'.join(str(unit_rf_df.groupby('Source').count()['x']).split('\\n')[:-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/unit_names', unit_names.astype(bytes))\n",
    "unit_rf_df.to_hdf(output_path, analysis_name+'/rf_per_unit', mode='a', format='table', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find matched saccades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(rsc_path, 'r') as f:\n",
    "    fix1_sel, fix2_sel = f['self_consistency/saccade_selection'][()]\n",
    "    return_thres = f['self_consistency/config/return_criterion/return_thres'][()]\n",
    "    v = f['self_consistency/return_pairs/'+match_test[1]][()]\n",
    "    assert v.size\n",
    "    v = np.sort(v, axis=1)\n",
    "    v = v[np.lexsort(tuple(v.T))]\n",
    "    match_return_pairs = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = np.random.default_rng(random_seed)\n",
    "pair_df = pd.DataFrame(np.concatenate([match_return_pairs, match_return_pairs[:,::-1]], axis=0)).set_index(0)\n",
    "if match_test[1] == 'previous':\n",
    "    src_fix_sel, other_fix_sel = fix1_sel, fix2_sel\n",
    "else:\n",
    "    src_fix_sel, other_fix_sel = fix2_sel, fix1_sel\n",
    "matched = np.zeros(fix1_sel.size, dtype=bool)\n",
    "match_ips = []\n",
    "\n",
    "for i, (src_ifix, other_ifix) in enumerate(zip(src_fix_sel, other_fix_sel)):\n",
    "    try:\n",
    "        match_ip_opts = pair_df.loc[[i],1].values\n",
    "    except KeyError:\n",
    "        continue\n",
    "    assert len(set(match_ip_opts)) == len(match_ip_opts)\n",
    "    match_ip_opts = rg.permuted(match_ip_opts)\n",
    "    for ip in match_ip_opts:\n",
    "        match_other_ifix = other_fix_sel[ip]\n",
    "        other_xy, match_other_xy = fix_df.iloc[[other_ifix, match_other_ifix]]\\\n",
    "            [['Relative X', 'Relative Y']].values\n",
    "        if np.linalg.norm(other_xy - match_other_xy) >= min_sep:\n",
    "            matched[i] = True\n",
    "            match_ips.append(ip)\n",
    "            break\n",
    "\n",
    "print(f'{matched.sum()} of {matched.size} ({matched.mean()*100:.1f}%) saccades had matched {match_test[1]} fixation '\n",
    "      f'and min. sep. >= {min_sep} dva for the unmatched fixation')\n",
    "match_fix1_sel = fix1_sel[match_ips]\n",
    "match_fix2_sel = fix2_sel[match_ips]\n",
    "fix1_sel = fix1_sel[matched]\n",
    "fix2_sel = fix2_sel[matched]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "cols = ['Relative X', 'Relative Y']\n",
    "xy1 = fix_df.iloc[fix1_sel][cols].values\n",
    "mxy1 = fix_df.iloc[match_fix1_sel][cols].values\n",
    "xy2 = fix_df.iloc[fix2_sel][cols].values\n",
    "mxy2 = fix_df.iloc[match_fix2_sel][cols].values\n",
    "d1 = np.linalg.norm(xy1 - mxy1, axis=1)\n",
    "d2 = np.linalg.norm(xy2 - mxy2, axis=1)\n",
    "if match_test[1] != 'previous':\n",
    "    d1, d2 = d2, d1\n",
    "assert (d1 < return_thres).all()\n",
    "assert (d2 >= min_sep).all()\n",
    "for sel, msel in ((fix1_sel, match_fix1_sel), (fix2_sel, match_fix2_sel)):\n",
    "    assert (fix_df.iloc[sel]['Image filename'].values == fix_df.iloc[msel]['Image filename'].values).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = analysis_name + '/config/match_criterion/'\n",
    "save_results(group+'return_thres', min_sep)\n",
    "\n",
    "group = analysis_name + '/saccade_selection/'\n",
    "save_results(group+'fixation_indices', np.array([fix1_sel, fix2_sel]))\n",
    "save_results(group+'match_saccades/fixation_indices', np.array([match_fix1_sel, match_fix2_sel]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get aligned responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(sdf_path, 'r') as f:\n",
    "    dset = f['sdf']\n",
    "    all_unit_names = list(dset.attrs['unit_names'].astype(str))\n",
    "    unit_sel = np.array([v in unit_names for v in all_unit_names])\n",
    "    sdf = dset[()][:,unit_sel]\n",
    "\n",
    "n_neur = sdf.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_win > 0:\n",
    "    if np.isfinite(t_loc):\n",
    "        timing = pd.DataFrame(data={'Index': np.arange(len(unit_names)), 'Timing': np.full(unit_names.size, t_loc)})\n",
    "    else:\n",
    "        timing = pd.read_csv(timing_path).set_index('Session').loc[[sess_name]].set_index('Name')\n",
    "        m = pd.Series(unit_names).isin(timing.index)\n",
    "        assert m.all(), f'missing timing values for {(~m).sum()} of {m.size} units'\n",
    "        assert not timing.index.has_duplicates\n",
    "        timing = timing.loc[unit_names].reset_index()\n",
    "        assert len(timing) == len(unit_names)\n",
    "        timing = timing[[t_col, 'Source']].rename(columns={t_col: 'Timing'})\n",
    "        timing['Timing'] += t_offset\n",
    "        timing['Index'] = np.arange(len(timing))\n",
    "        print('Num units using RF fit from each source:')\n",
    "        print('\\t' + '\\n\\t'.join(str(timing.groupby('Source').count()['Timing']).split('\\n')[:-1]))\n",
    "\n",
    "    # for quick illustrative plotting only; does not affect response timing\n",
    "    ts += timing['Timing'].values.mean()\n",
    "\n",
    "    timing.to_hdf(output_path, analysis_name+'/timing_per_unit', mode='a', format='table', complevel=9, complib='zlib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if t_aln == 'sacc_on':\n",
    "    t0s = fix_df.iloc[fix1_sel][['Time', 'Duration']].values.sum(1)\n",
    "    mt0s = fix_df.iloc[match_fix1_sel][['Time', 'Duration']].values.sum(1)\n",
    "else:\n",
    "    t0s = fix_df.iloc[fix2_sel]['Time'].values\n",
    "    mt0s = fix_df.iloc[match_fix2_sel]['Time'].values\n",
    "\n",
    "resps = np.empty_like(sdf, shape=(fix2_sel.size, ts.size, sdf.shape[-1]))\n",
    "mresps = np.empty_like(resps)\n",
    "\n",
    "if t_win > 0:\n",
    "    t_win_ = np.array([0, t_win])\n",
    "    lat_groups = [(dt, df_['Index'].values) for dt, df_ in timing.groupby('Timing')]\n",
    "    for resps_, t0s_ in ((resps, t0s), (mresps, mt0s)):\n",
    "        for i, t in enumerate(t0s_):\n",
    "            for dt, usel in lat_groups:\n",
    "                s = slice(*np.round(t+dt+t_win_).astype(int))\n",
    "                resps_[i,0,usel] = sdf[s,usel].mean(0)\n",
    "else:\n",
    "    for resps_, t0s_ in ((resps, t0s), (mresps, mt0s)):\n",
    "        for i, t in enumerate(t0s_):\n",
    "            ts_ = np.round(t+ts).astype(int)\n",
    "            resps_[i] = sdf[ts_,:]\n",
    "\n",
    "del sdf\n",
    "\n",
    "Y = standardize(resps)\n",
    "match_Y = standardize(mresps)\n",
    "\n",
    "del resps, mresps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define splits (group k-fold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imfns = fix_df.iloc[fix2_sel]['Image filename']\n",
    "\n",
    "splits, train_mask = cv_split_by_image(\n",
    "    imfns, n_splits,\n",
    "    group_kfold=group_kfold, random_seed=random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(analysis_name+'/fix_is_train', train_mask, attrs=dict(\n",
    "    dims=np.array(['split', 'fixation'], dtype=np.bytes_),\n",
    "    random_seed=random_seed,\n",
    "    group_kfold=group_kfold))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load pre-computed model reprs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5.File(feats_path, 'r') as f:\n",
    "    patch_locs_x = f['config/patch_grid/x_locs'][()]\n",
    "    patch_locs_y = f['config/patch_grid/y_locs'][()]\n",
    "    patch_step = float(f['config/patch_grid/step'][()])\n",
    "\n",
    "    bg_feats = f['feats/bg'][()]\n",
    "\n",
    "    all_md5s = f['md5'][()].astype(str)\n",
    "    all_md5s = pd.Series(index=all_md5s, data=np.arange(len(all_md5s)), name='Index in file')\n",
    "    idc = all_md5s.loc[md5_catalog['MD5'].values]\n",
    "    md5_catalog['Index'] = np.arange(len(md5_catalog))\n",
    "\n",
    "    patch_grid_feats = np.empty(shape=(idc.size,patch_locs_x.size,patch_locs_y.size)+bg_feats.shape, dtype=bg_feats.dtype)\n",
    "    for ii, i in enumerate(idc.values):\n",
    "        patch_grid_feats[ii] = f['feats/patch_grid'][i]  # shape (n_patches_x, n_patches_y,) + feats_shape\n",
    "\n",
    "    copy_group(f, 'config', analysis_name+'/config/feats')\n",
    "\n",
    "feats_shape = bg_feats.shape\n",
    "print('Features shape:', feats_shape)\n",
    "print('Patch-grid features shape:', patch_grid_feats.shape)\n",
    "\n",
    "iims = np.array([\n",
    "    md5_catalog.loc[(row['Image subdir'], row['Image filename']), 'Index']\n",
    "    for i, (_, row) in enumerate(fix_df.iloc[fix2_sel].iterrows())])\n",
    "patch_bins_x = np.concatenate([\n",
    "    patch_locs_x-patch_step/2, [patch_locs_x[-1]+patch_step/2]])\n",
    "patch_bins_y = np.concatenate([\n",
    "    patch_locs_y-patch_step/2, [patch_locs_y[-1]+patch_step/2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recon_stim(*args, **kwargs):\n",
    "    return get_patches_from_grid(\n",
    "        *args, patch_bins_x=patch_bins_x, patch_bins_y=patch_bins_y, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xys_fix1 = fix_df.iloc[fix1_sel][['Relative X', 'Relative Y']].values.astype(float)\n",
    "xys_fix2 = fix_df.iloc[fix2_sel][['Relative X', 'Relative Y']].values.astype(float)\n",
    "sacc_vecs = xys_fix2 - xys_fix1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_win = len(ts)\n",
    "n_split = n_splits\n",
    "n_loc = len(rf_rlocs)\n",
    "n_unit = len(unit_names)\n",
    "n_fix = fix2_sel.size\n",
    "\n",
    "cv_corrs = np.full((2, n_win, n_loc, n_unit), np.nan, dtype=np.float32)\n",
    "cv_corrs_per_split = np.full((2, n_win, n_split, n_loc, n_unit), np.nan, dtype=np.float32)\n",
    "if n_perm > 0:\n",
    "    dsamps = np.empty_like(cv_corrs_per_split[0])\n",
    "    dperms = np.empty_like(dsamps, shape=(n_perm,)+dsamps.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_rf_df['Index'] = np.arange(len(unit_rf_df))\n",
    "gb = unit_rf_df.groupby(['x', 'y'])\n",
    "\n",
    "for (igroup, ((rf_x, rf_y), df_)), (il, rloc) in tqdm(\n",
    "        itertools.product(enumerate(gb), enumerate(rf_rlocs)),\n",
    "        total=len(gb)*len(rf_rlocs)):\n",
    "    usel = df_['Index'].values\n",
    "    xys = xys_fix1 + [rf_x, rf_y] + rloc * sacc_vecs\n",
    "    X = standardize(recon_stim(iims, xys, patch_grid_feats, bg_feats))\n",
    "\n",
    "    Yu = Y[...,usel]\n",
    "    match_Yu = match_Y[...,usel]\n",
    "\n",
    "    for i, Y_ in enumerate((Yu, match_Yu)):\n",
    "        corr, _, corr_pers, _ = cv_ridge_predict_eval(X, Y_, splits, alpha=ridge_alpha)\n",
    "        cv_corrs[i,:,il,usel] = corr.T\n",
    "        cv_corrs_per_split[i,:,:,il,usel] = corr_pers.transpose(2,0,1)\n",
    "\n",
    "    if n_perm > 0:\n",
    "        dsamps[...,il,usel] = np.diff(cv_corrs_per_split[...,il,usel], axis=0)[0]\n",
    "\n",
    "        rg = np.random.default_rng(random_seed)  # fine, even preferable, to use same randomization across loops\n",
    "        both_Yu = np.concatenate([Yu, match_Yu], axis=0)\n",
    "        idc = np.arange(len(both_Yu))\n",
    "        dperms_ = np.empty((n_perm, n_win, n_split, usel.size))\n",
    "        for iperm in range(n_perm):\n",
    "            idc = rg.permuted(idc)\n",
    "            idc1, idc2 = np.split(idc, 2)\n",
    "            dperms_[iperm] = (\n",
    "                cv_ridge_predict_eval(X, both_Yu[idc2], splits, alpha=ridge_alpha)[2]\n",
    "                - cv_ridge_predict_eval(X, both_Yu[idc1], splits, alpha=ridge_alpha)[2])\n",
    "        dperms[:,...,il,usel] = dperms_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate permutation p-values\n",
    "if n_perm > 0:\n",
    "    alt = match_test[3]\n",
    "    if match_test[2] is not None:\n",
    "        ilt = list(rf_rlocs).index(match_test[2])\n",
    "        dsamps -= dsamps[...,[ilt],:]\n",
    "        dperms -= dperms[...,[ilt],:]\n",
    "    else:\n",
    "        ilt = None\n",
    "    if alt == 'two-sided':\n",
    "        d = np.abs(dsamps) - np.abs(dperms)\n",
    "    elif alt == 'less':      # original < match\n",
    "        d = dsamps - dperms\n",
    "    else:                    # original > match\n",
    "        d = dperms - dsamps  # d(2)samp ~ match - original < 0\n",
    "    d = np.ma.masked_invalid(d)  # correlation can be nan\n",
    "    # splits do not meaningfully differ w.r.t. the test, so pool them\n",
    "    d = d.transpose(0,2,1,3,4).reshape(-1, d.shape[1], d.shape[3], d.shape[4])\n",
    "    p = 1 - (d > 0).mean(0)\n",
    "    # permutation p is lower-bound by (valid) num perms\n",
    "    pvals = np.clip(p, 1 / (1 + (~d.mask).sum(0)), None).filled(np.nan)\n",
    "    if ilt is not None:\n",
    "        pvals[:,ilt,:] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dims = np.array(('condition', 'time', 'split', 'pos', 'unit'))\n",
    "coords = dict((\n",
    "    ('condition', np.array(('original', 'match'))),\n",
    "    ('time', ts),\n",
    "    ('pos', rf_rlocs),\n",
    "    ('unit', unit_names)))\n",
    "attrs = dict(\n",
    "    n_sacc=fix1_sel.size,\n",
    "    group_kfold=int(group_kfold),\n",
    "    feat_shape=feats_shape,\n",
    "    ridge_alpha=ridge_alpha,\n",
    "    t_aln=t_aln)\n",
    "\n",
    "q = lambda x: quantize(x, 3)\n",
    "data_vars = {\n",
    "    'corr': (dims[[0,1,3,4]], q(cv_corrs)),\n",
    "    'corr_per_split': (dims, q(cv_corrs_per_split))}\n",
    "if n_perm > 0:\n",
    "    data_vars['p-value'] = (dims[[1,3,4]], pvals)\n",
    "dataset = xr.Dataset(data_vars, coords=coords, attrs=attrs)\n",
    "\n",
    "compr = dict(zlib=True, complevel=9)\n",
    "encoding = {\n",
    "    k: dict(chunksizes=v.shape, **compr)\n",
    "    for k, v in dataset.data_vars.items()}\n",
    "dataset.to_netcdf(\n",
    "    output_path, group=analysis_name+'/data',\n",
    "    mode='a', engine='h5netcdf', encoding=encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wrap up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(f'progress_report/{analysis_name}/all_done', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "%watermark\n",
    "%watermark -vm --iversions -rbg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic visualization of results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = np.nanmean(cv_corrs, -1)\n",
    "for l, c in zip((0, 1, .5), ('tab:blue', 'tab:orange', 'tab:green')):\n",
    "    i = np.argmin(np.abs(rf_rlocs - l))\n",
    "    for j, ls in enumerate(('-', '--')):\n",
    "        plt.plot(ts, ms[j,:,i], label=rf_rlocs[i], color=c, ls=ls,\n",
    "                 marker='.', markerfacecolor=(c,'none')[j], markeredgecolor=('none',c)[j])\n",
    "plt.legend(title='Norm. pos. on sacc.')\n",
    "plt.xlabel(f'Time rel. {t_aln.replace(\"_\", \" \")}, ms')\n",
    "plt.ylabel('Model fit, Pearson\\'s r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vr = abs(np.nanpercentile(ms, 99.5))\n",
    "\n",
    "t_step_ = t_win if t_win > 0 else t_step\n",
    "x_step_ = rloc_step\n",
    "ext = (ts[0]-t_step_/2, ts[-1]+t_step_/2, rf_rlocs[0]-x_step_/2, rf_rlocs[-1]+x_step_/2)\n",
    "aspect = np.array(ext).reshape(2,2).ptp(1)\n",
    "aspect = aspect[0] / aspect[1] * .618\n",
    "\n",
    "for i in range(2):\n",
    "    plt.figure()\n",
    "    cm = plt.imshow(\n",
    "        ms[i].T, extent=ext,\n",
    "        origin='lower', aspect=aspect,\n",
    "        cmap='RdGy', vmin=-vr, vmax=vr)\n",
    "    cb = plt.colorbar(cm, fraction=.05, aspect=10)\n",
    "    cb.ax.set_ylabel('Model fit, Pearson\\'s r')\n",
    "    plt.ylabel('Norm. pos. on sacc.')\n",
    "    plt.xlabel(f'Time rel. {t_aln.replace(\"_\", \" \")}, ms')\n",
    "    plt.title(('Original','Match')[i] + '-saccade responses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_perm > 0:\n",
    "    ms = (pvals < 0.01).mean(2)\n",
    "    plt.figure()\n",
    "    cm = plt.imshow(\n",
    "        ms.T, extent=ext,\n",
    "        origin='lower', aspect=aspect,\n",
    "        cmap='gray_r', vmin=0, vmax=1)\n",
    "    cb = plt.colorbar(cm, fraction=.05, aspect=10)\n",
    "    cb.ax.set_ylabel('Frac. sig. neurons')\n",
    "    plt.ylabel('Norm. pos. on sacc.')\n",
    "    plt.xlabel(f'Time rel. {t_aln.replace(\"_\", \" \")}, ms');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if n_perm > 0:\n",
    "    ms = np.median(pvals, 2)\n",
    "    if np.isfinite(ms).any():\n",
    "        vm = np.nanmax(ms)\n",
    "        plt.figure()\n",
    "        cm = plt.imshow(\n",
    "            ms.T, extent=ext,\n",
    "            origin='lower', aspect=aspect,\n",
    "            cmap='gray_r', vmin=0, vmax=vm)\n",
    "        cb = plt.colorbar(cm, fraction=.05, aspect=10)\n",
    "        cb.ax.set_ylabel('Median p-value')\n",
    "        plt.gca().set_facecolor('firebrick')\n",
    "        plt.ylabel('Norm. pos. on sacc.')\n",
    "        plt.xlabel(f'Time rel. {t_aln.replace(\"_\", \" \")}, ms');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
